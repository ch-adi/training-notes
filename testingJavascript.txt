

Fundamentals of Testing In Javascript
----------------------------------------------

Say We have a bug in our sum function. To make sure it enver happens again we write a test. Essentially, . A test is code that throws an error when the actual result of something does not match the expected output. 

    const sum = (a, b) => a - b;

    const result = sum(3, 7);
    const expected = 10;
    if (result !== expected){
        throw new Error(`${result} is not equal to ${expected}`)
    }

The  job of a Javascript testing framework is to make that error message as useful as possible so we can quickly identify what the problem is and fix it.

To make this code less imperative, and more declarative, we can create an assertion library.

    function expect(actual) {
      return {
        toBe(expected) {
          if (actual !== expected) {
            throw new Error(`${actual} is not equal to ${expected}`)
          }
        }
      }
    }

    const result = sum(3, 7);
    const expected = 10;
    expect(result).toBe(expected);

We can add other assertions to the library object. Eg:

    toEqual(expected) {}
    toBeGreaterThan(expected) {}
    
One problem with this approach is that as soon as one test fails, and throws an error, no other tests will be run. When finding the cause of a test failure, knowing the state of other tests can be useful. In fact even some clear logging around which test actually failed is important.    
    
    function test(title, callback) {
      try {
        callback()
        console.log(`✓ ${title}`)
      } catch (error) {
        console.error(`✕ ${title}`)
        console.error(error)
      }
    }
    
    test('sum adds numbers', () => {
      const result = sum(3, 7)
      const expected = 10
      expect(result).toBe(expected)
    })
    
A test library should quickly identify what is broken so that we can make a quick fix.

The next step is to handle async code. Even if we make our callback async the test function will complete and then later an unhandled promise rejection error will occur along with the failed test info.

    test('sumAsync adds numbers asynchronously', async () => {
      const result = await sumAsync(3, 7)
      const expected = 10
      expect(result).toBe(expected)
    })

    > (node:11815) UnhandledPromiseRejectionWarning: Unhandled promise rejection (rejection id: 1) ... Error: -4 is not equal to 10 ...
    
The answer is to add an await to our test function. We need to wait for the callback to finish:

    async function test(title, callback) {
      try {
        await callback()
        console.log(`✓ ${title}`)
      } catch (error) {
        console.error(`✕ ${title}`)
        console.error(error)
      }
    }
    
 Many test libraries are set up as global variables. We can do that my moving our library into another file, setup-global.js. We add some exports at the bottom of this file and then we can run our tests:
 
    // bottom of setup-global.js
    global.test = test
    global.expect = expect
    
    > node --require ./setup-globals.js lessons/globals.js
 
 
 
Static Analysis Testing
--------------------------------------------

Static analysis involves no dynamic execution of the software under test and can detect possible defects in an early stage, before running the program. Static analysis can be done by a machine to automatically “walk through” the source code and detect noncomplying rules. Static analysis can also be performed by a person who would review the code to ensure proper coding standards and conventions are used to construct the program. This is often called Code Review.

Quality attributes that can be the focus of static analysis:
 - Reliability
 - Maintainability  (also inlcludes comments)
 - Testability  (are there tests?)
 - Re-usability
 - Portability
 - Efficiency
 
Once tool is eslint. You can set up rules in .eslintrc

    {
      "parserOptions": {
        "ecmaVersion": "2018"
      },
       "extends": [
        "eslint:recommended"    // automatically use the reccomended set of rules
      ],
      "rules": {
        "valid-typeof": "error" // will break the build. Can use "warn" instead. Or, can use "off" to disable the rule.
      }
    }

So you don't have to run $ npm eslint src all the time you can add eslint to your package.json:

    "scripts": {
        "lint": "eslint src"
    },
    
You can also just add an eslint extension to your code editor. Since you don't have to run anything you get a faster feedback loop.


Code formatting tools, like perttier are also available.Such tools limit arguments with others and tidy the code.

    $ npx prettier src/example.js  // just see the result
    $ npx prettier --write src/example.js  // write the result to the file, or:
    
    "scripts": {
        "lint": "eslint src",
        "format": "prettier --write \"**/*.+(js|jsx|json|yml|yaml|css|less|scss|ts|tsx|md|mdx|graphql|vue)\""
    },
    
There may also be some code editor support for prettier. If so you should enable format on save in the editor's settings. You can also change prettier's setting with a .prettierrc file. Go to https://prettier.io/playground/ to try out and generate some config options. A .prettierignore can be used to ignore some other file types like node modules.

If there are conflicts between prettier and eslint then try this module: npm install --save-dev eslint-config-prettier, cnad update extends in .eslintrc:

  "extends": [
    "eslint:recommended", "eslint-config-prettier"
  ],
  
 [this example conintinues into the node scripts section of the node training file]
 
 
 
 Mocking
 ------------------------------------------------
When running unit tests, you don’t want to actually make network requests or charge real credit cards. This can easily get slow and flaky. Instead we want some aspects of the code such as inputs into a function under test to be deterministic. 

One way to do this is 'monkey-patching'. We import some functionality but assign some deterministic replacement. Then we properly dispose of that replacement.

    const originalGetWinner = utils.getWinner    // we will overwrite getWinner, which returns a random result used within thumbWar()
    utils.getWinner = (p1, p2) => p1            //  p1 will always be the return value used within thumbWar().

    const winner = thumbWar('Kent C. Dodds', 'Ken Wheeler')
    assert.strictEqual(winner, 'Kent C. Dodds')    // if the rest of thumbWar is working then p1 should now always be the result

    utils.getWinner = originalGetWinner   // properly dispose of our version of getWinner.
    

In this case it would also be great to test how getWinner is being called. For example, what if it was not called with the correct arguments. Say only p1 and not p2. The test would still pass but the implementation would be wrong.
        
     test('returns winner', () => {
      const originalGetWinner = utils.getWinner
      utils.getWinner = jest.fn((p1, p2) => p1)

      const winner = thumbWar('Kent C. Dodds', 'Ken Wheeler')
      expect(winner).toBe('Kent C. Dodds)
      expect(utils.getWinner).toHaveBeenCalledTimes(2)
      expect(utils.getWinner).toHaveBeenCalledWith('Kent C. Dodds', 'Ken Wheeler')  // picks up the implementation break
    })


Or if we expect the mocked function to have been called with different arguments:

  expect(utils.getWinner).toHaveBeenNthCalledWith(1, 'Kent C. Dodds', 'Ken Wheeler')  // arguments for first call - '1'
  expect(utils.getWinner).toHaveBeenNthCalledWith(2, 'Kent C. Dodds', 'Henry Jones')
  
Inspecting the mocked function object actually allows us to replace many of these expectations. For example, you can log it out and see the calls:

    console.log(utils.getWinner.mock.calls); // see and array of 'calls'
    
 Now all we need to test is this:
 
     expect(utils.getWinner.mock.calls).toEqual([
      [ 'Kent C. Dodds', 'Ken Wheeler' ],
      [ 'Kent C. Dodds', 'Ken Wheeler' ]
    ]);
    
That is, we know there have been two calls and what the arguments for those calls have been.


We could make our own mocking function to get the same result:

    function fn(impl) {
      const mockFn = (...args) => {
        mockFn.mock.calls.push(args)
        return impl(...args)
      }
      mockFn.mock = {calls: []}
      return mockFn
    }

    assert.strictEqual(winner, 'Kent C. Dodds')
    assert.deepStrictEqual(utils.getWinner.mock.calls, [
      [ 'Kent C. Dodds', 'Ken Wheeler' ],
      [ 'Kent C. Dodds', 'Ken Wheeler' ]
    ])
        
This fn function accepts an implementation and returns a function that calls that implementation with all of those arguments.

It also keeps track of all the arguments that it's called with so that we can assert how that function is called, allowing us to catch issues in our integration with the getWinner function.
 
Having to dispose of our version of the mocked function can be tiresome. The workout is to use spyOn and restore. spyOn takes an object and a method:

    test('returns winner', () => {
      jest.spyOn(utils, 'getWinner')   // instead of `const originalGetWinner = utils.getWinner`
      utils.getWinner.mockImplementation((p1, p2) => p2) // replaces utils.getWinner = jest.fn((p1, p2) => p2)

      ...

      // cleanup
      utils.getWinner.mockRestore()  // instead of `utils.getWinner = originalGetWinner`
    
 To make our own version we first need a mock function factory that takes a default empty function:
 
    function fn(impl = () => {}) {
      const mockFn = (...args) => {
        mockFn.mock.calls.push(args)
        return impl(...args)
      }
      mockFn.mock = {calls: []}
      mockFn.mockImplementation = newImpl => (impl = newImpl)  // this allows our tests to be deterministic
      return mockFn
    }

And spyOn:

    function spyOn(obj, prop) {
      const originalValue = obj[prop]
      obj[prop] = fn()
      obj[prop].mockRestore = () => (obj[prop] = originalValue)
    }
    
SpyOn is resonisble for tracking the original value. Then it provides a mockRestore function, which we can use to restore the originalValue to that object.
