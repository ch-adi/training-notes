

Fundamentals of Testing In Javascript
----------------------------------------------

Say We have a bug in our sum function. To make sure it enver happens again we write a test. Essentially, . A test is code that throws an error when the actual result of something does not match the expected output. 

    const sum = (a, b) => a - b;

    const result = sum(3, 7);
    const expected = 10;
    if (result !== expected){
        throw new Error(`${result} is not equal to ${expected}`)
    }

The  job of a Javascript testing framework is to make that error message as useful as possible so we can quickly identify what the problem is and fix it.

To make this code less imperative, and more declarative, we can create an assertion library.

    function expect(actual) {
      return {
        toBe(expected) {
          if (actual !== expected) {
            throw new Error(`${actual} is not equal to ${expected}`)
          }
        }
      }
    }

    const result = sum(3, 7);
    const expected = 10;
    expect(result).toBe(expected);

We can add other assertions to the library object. Eg:

    toEqual(expected) {}
    toBeGreaterThan(expected) {}
    
One problem with this approach is that as soon as one test fails, and throws an error, no other tests will be run. When finding the cause of a test failure, knowing the state of other tests can be useful. In fact even some clear logging around which test actually failed is important.    
    
    function test(title, callback) {
      try {
        callback()
        console.log(`✓ ${title}`)
      } catch (error) {
        console.error(`✕ ${title}`)
        console.error(error)
      }
    }
    
    test('sum adds numbers', () => {
      const result = sum(3, 7)
      const expected = 10
      expect(result).toBe(expected)
    })
    
A test library should quickly identify what is broken so that we can make a quick fix.

The next step is to handle async code. Even if we make our callback async the test function will complete and then later an unhandled promise rejection error will occur along with the failed test info.

    test('sumAsync adds numbers asynchronously', async () => {
      const result = await sumAsync(3, 7)
      const expected = 10
      expect(result).toBe(expected)
    })

    > (node:11815) UnhandledPromiseRejectionWarning: Unhandled promise rejection (rejection id: 1) ... Error: -4 is not equal to 10 ...
    
The answer is to add an await to our test function. We need to wait for the callback to finish:

    async function test(title, callback) {
      try {
        await callback()
        console.log(`✓ ${title}`)
      } catch (error) {
        console.error(`✕ ${title}`)
        console.error(error)
      }
    }
    
 Many test libraries are set up as global variables. We can do that my moving our library into another file, setup-global.js. We add some exports at the bottom of this file and then we can run our tests:
 
    // bottom of setup-global.js
    global.test = test
    global.expect = expect
    
    > node --require ./setup-globals.js lessons/globals.js
 
 
 
Static Analysis Testing
--------------------------------------------

Static analysis involves no dynamic execution of the software under test and can detect possible defects in an early stage, before running the program. Static analysis can be done by a machine to automatically “walk through” the source code and detect noncomplying rules. Static analysis can also be performed by a person who would review the code to ensure proper coding standards and conventions are used to construct the program. This is often called Code Review.

Quality attributes that can be the focus of static analysis:
 - Reliability
 - Maintainability  (also inlcludes comments)
 - Testability  (are there tests?)
 - Re-usability
 - Portability
 - Efficiency
 
Once tool is eslint. You can set up rules in .eslintrc

    {
      "parserOptions": {
        "ecmaVersion": "2018"
      },
       "extends": [
        "eslint:recommended"    // automatically use the reccomended set of rules
      ],
      "rules": {
        "valid-typeof": "error" // will break the build. Can use "warn" instead. Or, can use "off" to disable the rule.
      }
    }

So you don't have to run $ npm eslint src all the time you can add eslint to your package.json:

    "scripts": {
        "lint": "eslint src"
    },
    
You can also just add an eslint extension to your code editor. Since you don't have to run anything you get a faster feedback loop.


Code formatting tools, like perttier are also available.Such tools limit arguments with others and tidy the code.

    $ npx prettier src/example.js  // just see the result
    $ npx prettier --write src/example.js  // write the result to the file, or:
    
    "scripts": {
        "lint": "eslint src",
        "format": "prettier --write \"**/*.+(js|jsx|json|yml|yaml|css|less|scss|ts|tsx|md|mdx|graphql|vue)\""
    },
    
There may also be some code editor support for prettier. If so you should enable format on save in the editor's settings. You can also change prettier's setting with a .prettierrc file. Go to https://prettier.io/playground/ to try out and generate some config options. A .prettierignore can be used to ignore some other file types like node modules.

If there are conflicts between prettier and eslint then try this module: npm install --save-dev eslint-config-prettier, cnad update extends in .eslintrc:

  "extends": [
    "eslint:recommended", "eslint-config-prettier"
  ],
  
 [this example conintinues into the node scripts section of the node training file]
 
 
 
 Mocking
 ------------------------------------------------
When running unit tests, you don’t want to actually make network requests or charge real credit cards. This can easily get slow and flaky. Instead we want some aspects of the code such as inputs into a function under test to be deterministic. 

One way to do this is 'monkey-patching'. We import some functionality but assign some deterministic replacement. Then we properly dispose of that replacement.

    const originalGetWinner = utils.getWinner    // we will overwrite getWinner, which returns a random result used within thumbWar()
    utils.getWinner = (p1, p2) => p1            //  p1 will always be the return value used within thumbWar().

    const winner = thumbWar('Kent C. Dodds', 'Ken Wheeler')
    assert.strictEqual(winner, 'Kent C. Dodds')    // if the rest of thumbWar is working then p1 should now always be the result

    utils.getWinner = originalGetWinner   // properly dispose of our version of getWinner i.e. fix up the original object.
    

In this case it would also be great to test how getWinner is being called. For example, what if it was not called with the correct arguments. Say only p1 and not p2. The test would still pass but the implementation would be wrong.
        
     test('returns winner', () => {
      const originalGetWinner = utils.getWinner
      utils.getWinner = jest.fn((p1, p2) => p1)

      const winner = thumbWar('Kent C. Dodds', 'Ken Wheeler')
      expect(winner).toBe('Kent C. Dodds)
      expect(utils.getWinner).toHaveBeenCalledTimes(2)
      expect(utils.getWinner).toHaveBeenCalledWith('Kent C. Dodds', 'Ken Wheeler')  // picks up the implementation break
    })


Or if we expect the mocked function to have been called with different arguments:

  expect(utils.getWinner).toHaveBeenNthCalledWith(1, 'Kent C. Dodds', 'Ken Wheeler')  // arguments for first call - '1'
  expect(utils.getWinner).toHaveBeenNthCalledWith(2, 'Kent C. Dodds', 'Henry Jones')
  
Inspecting the mocked function object actually allows us to replace many of these expectations. For example, you can log it out and see the calls:

    console.log(utils.getWinner.mock.calls); // see and array of 'calls'
    
 Now all we need to test is this:
 
     expect(utils.getWinner.mock.calls).toEqual([
      [ 'Kent C. Dodds', 'Ken Wheeler' ],
      [ 'Kent C. Dodds', 'Ken Wheeler' ]
    ]);
    
That is, we know there have been two calls and what the arguments for those calls have been, with just one assertion.


We could make our own mocking function to get the same result:

    function fn(impl) {
      const mockFn = (...args) => {
        mockFn.mock.calls.push(args)
        return impl(...args)
      }
      mockFn.mock = {calls: []}
      return mockFn
    }
    
    const originalGetWinner = utils.getWinner
    utils.getWinner = fn((p1, p2) => p1)

    assert.strictEqual(winner, 'Kent C. Dodds')
    assert.deepStrictEqual(utils.getWinner.mock.calls, [
      [ 'Kent C. Dodds', 'Ken Wheeler' ],
      [ 'Kent C. Dodds', 'Ken Wheeler' ]
    ])
        
This fn function accepts an implementation and returns a function that calls that implementation with all of those arguments.

It also keeps track of all the arguments that it's called with so that we can assert how that function is called, allowing us to catch issues in our integration with the getWinner function.
 
Having to dispose of our version of the mocked function can be tiresome. The workaround is to use spyOn and restore. spyOn takes an object and a method:

    test('returns winner', () => {
      jest.spyOn(utils, 'getWinner')   // instead of `const originalGetWinner = utils.getWinner`
      utils.getWinner.mockImplementation((p1, p2) => p2) // replaces utils.getWinner = jest.fn((p1, p2) => p2)

      ...

      // cleanup
      utils.getWinner.mockRestore()  // instead of `utils.getWinner = originalGetWinner`
    
    
 To make our own version we first need to update our mock function factory with a mockImplementation method:
 
    function fn(impl = () => {}) {
      const mockFn = (...args) => {
        mockFn.mock.calls.push(args)
        return impl(...args)
      }
      mockFn.mock = {calls: []}
      mockFn.mockImplementation = newImpl => (impl = newImpl)  // this allows our tests to be deterministic
      return mockFn
    }

And spyOn:

    function spyOn(obj, prop) {
      const originalValue = obj[prop]
      obj[prop] = fn()
      obj[prop].mockRestore = () => (obj[prop] = originalValue)
    }
    
So, SpyOn is resonisble for tracking the original value and then restoring the originalValue.


So far our monkey-patching has been working because we have been using common JS. It does not work with es6 modules. However, jest allows you to mock an entire module with the jest.mock API. The first argument is the module being mocked, realtive to the call to jest.mock. The 2nd argument is a module factory function that will return a mocked version of the module.

    jest.mock('../utils', () => {
      return {
        getWinner: jest.fn((p1, p2) => p1)
      }
    })

    test('returns winner', () => {
        // remove spyon and mock implementation
      const winner = thumbWar('Kent C. Dodds', 'Ken Wheeler')
      expect(winner).toBe('Kent C. Dodds')
      expect(utilsMock.getWinner.mock.calls).toEqual([
        ['Kent C. Dodds', 'Ken Wheeler'],
        ['Kent C. Dodds', 'Ken Wheeler']
      ])

      // cleanup
      utils.getWinner.mockReset() // replace mockRestore with mockReset
    })
    
Also before Jest runs our code, it moves the jest.mock call up to the top of the file to ensure that the mock is in place before any of our modules are loaded. This is useful as other imports are often hoisted to the top of the file.
    
If you wanted to implement this yourself you would need the require cache. If you were to log it out you would see an object with many keys:

    console.log(require.cache);
    
    { '/Users/kdodds/Developer/js-mocking-fundamentals/node_modules/jest-worker/build/child.js':
      Module {
        id: '.',
        exports: {},
        parent: null,
        ...
      }
    }
    
Let's go ahead and get the utilsPath with require.resolve('../utils'). We'll then say require.cache at the utilsPath equals an object, and this object needs to resemble a module, so we'll say id is utilsPath, the filename is utilsPath, loaded is true, and exports is our mock, so we'll say getWinner is a call to our function with p1, p2 always returning p1. 
    
    const utilsPath = require.resolve('../utils')
    require.cache[utilsPath] = {
      id: utilsPath,
      filename: utilsPath,
      loaded: true,
      exports: {
        getWinner: fn((p1, p2) => p1)
      }
      
      // cleanup
       delete require.cache[utilsPath]
    }
 
With that, we can change things here a little bit. Let's put fn() up here at the top. We'll get rid of this spyOn function, we don't need that anymore. We'll get rid of our getWinner.mockImplementation call here.

We'll change our cleanup to delete require.cache at that utilsPath. That way any other modules that want to use the utils can do so without having trouble with our module mocking them out.

We could also preload the require.cache with the mock module that we want to have loaded when thumb-war requires the utils module.

    require.cache[utilsPath] = {
      id: utilsPath,
      filename: utilsPath,
      loaded: true,
      exports: {
        getWinner: fn((p1, p2) => p1)
      }
    }


If more than one test will need a mock of a module you can externalise your mock directory. Create a __mocks__ directory next to the file you wish to mock. Inside this directory create a file with the same name as the file to be mocked. 

For example, if we are to mock the getWinner method from the utils directory, the __mocks__/utils will look like:

    module.exports = {
      getWinner: jest.fn((p1, p2) => p1)
    }

In our test file we now need this:

    jest.mock('../utils')



Configuring Jest
-------------------------------------


Jest runs in node (via a some binaries in the .bin folder in node modules).

When Jest runs it automatically sets the node environment variable to 'test'. This allows us to change other configurations, such as the webpack config, when we are running tests. So, for example, in somehwere like .babelrc.js we can choose a to load mosules in a way that suits test but not production:

    const isTest = String(process.env.NODE_ENV) === 'test'
    
    presets: [
        ['@babel/preset-env', {modules: isTest ? 'commonjs' : false}],    


That's saying, "Hey, Babel preset env. When you come across a module, and if we're in the test environment, I want you to transpile that to commonjs so that it works in Node. Otherwise, we're probably building with webpack, and so, I don't want you to transpile that."

Eventhough Jest is running in node you can still log out 'window'. That is we can run tests in the browser too. Jest is intended to test JavaScript that runs in the browser or in node. This is because of a module called JSDOM. JSDOM will simulate this browser environment. There's a little bit of a performance hit for Jest to set up this JSDOM test environment. If you're just writing code that can run in Node then it's better to tell Jest to run that code in a Node test environment and not set up JSDOM.

You can force a node environment by passing on the argument env=node:

    npm t -- --env=node   // any tests that uses 'window' will fail with a warning now: ReferenceError: window is not defined
    
You can also set a node environment in a jest.config.js:

    module.exports = {
        testEnvironment: 'jest-environment-node',     // 'jest-environment-jsdom' if you do want JSDOM
    }

Say, when testing react componest with css files, you may get errors around css files imported as modules. Webpack has a loader (called 'style-loader') that handles modules. However in the node environment, node assumes that the imported statement for the css file is referring to a node module. We want to tell Jest that any time it sees a .css import we want it to resolve to some other module. We need a regex expression to map from css to a style mock. jest.config.js:

    module.exports = {
        testEnvironment: 'jest-environment-jsdom',
        moduleNameMapper: {
        '\\.css$': require.resolve('./test/style-mock.js'),
      }
    }

A style mock doesn't need to be anything special. We can just say module.exports equals a empty object. style-mock.js:

    module.exports = {}
    
Now, however, any tests that look at style wont show much:

    import 'react-testing-library/cleanup-after-each'
    import React from 'react'
    import {render} from 'react-testing-library'
    import AutoScalingText from '../auto-scaling-text'

    test('renders', () => {
      const {container} = render(<AutoScalingText />)   // get the container from this render
      console.log(container.innerHTML) 
    })
    
The log displays <div style="transform: scale(1,1);"></div>, but we don't see the className that is associated with the autoscalling text. 
    
    $ npm install --save-dev identity-obj-proxy
    
  
      moduleNameMapper: {
        '\\.module\\.css$': 'identity-obj-proxy',
        '\\.css$': require.resolve('./test/style-mock.js'),
      },
      
Now we log the property we are trying to access:

    <div class= "autoScalingText" style="transform: scale(1,1);"></div> 
    
Again, this is only a generated value because this is CSS modules. However it is a lot more helpful to have something like that there in our output, both when we're debugging and if we want to take a snapshot of this HTML.
      
Now, it's important to have these in the right order, because these both will match the module.css files, but we want any module.css to match the identity-obj-proxy instead.    
    
    
Snapshots

Snapshots creates a serialized value of the object we are expecting. In this case we will look at serialising and foratting DOM nodes. 
 
 ( aside: serialization (or serialisation) is the process of translating data structures or object state into a format that can be stored (for example, in a file or memory buffer) or transmitted (for example, across a network connection link) and reconstructed later (possibly in a different computer environment).) 
 
In jest, an object is serialized as a string (and pretty prints it).

    import 'react-testing-library/cleanup-after-each'
    import React from 'react'
    import {render} from 'react-testing-library'
    import CalculatorDisplay from '../calculator-display'

    test('mounts', () => {
      const {container} = render(<CalculatorDisplay value="0" />)
      
      // the value we will end up asserting in this test (checking it out for now)
      console.log(container.innerHTML)     // logs some HTML but it is untidy.
    })
    
We could actually snapshot container.innerHTML, but the problem with that is any change to any of these attributes would negate the entire snapshot would be harder to read diffs of the snapshot.   

If you only have one child that you're rendering, then it doesn't actually make any sense to snapshot that diff. You could actually snapshot the firstChild which will be the root node of the thing that you're rendering. For us that's this div here.

    calculator-display.js
    expect(container.firstChild).toMatchSnapshot()

In this way your snapshot would be a little less nested and so easier to read. The resultant snapshots are useful for people readding your commits and finding bugs.

One great feature of Jest snapshots is custom serialzers. And, someone has already done that for us with the emotion package:

    $ npm install --save-dev jest-emotion
    
    import {createSerializer} from 'jest-emotion'
    import * as emotion from 'emotion'
    
Then, we'll use the Jest API for adding a snapshot serializer:

    expect.addSnapshotSerializer(createSerializer(emotion))  // allows for a much nicer representation of class names
    
There are bunch of different snapshot serializers available on npm.

For example, there is a package called jest-serializer-path that removes absolute paths and normalized paths across all platforms in your Jest snapshots. Many snapshot serializer packages, like this one, actually expose the serializer itself rather than requiring you to create it in code, and for snapshot serializers like that, you can actually edit your Jest configuration and add a snapshotSerializers property with an array of path to packages.

jest.config.js:

    module.exports = {
      testEnvironment: 'jest-environment-jsdom',
      moduleNameMapper: {
        '\\.module\\.css$': 'identity-obj-proxy',
        '\\.css$': require.resolve('./test/style-mock.js'),
      },
      snapshotSerializers: []
    }
    
We have already covered static imports. Now we look at dynamic imports, which are not supported in node. Babel won't transpile dynamic imports to commonjs. 

We need to install a new package that will transpile this to commonjs so that our tests will work. We'll run npm install --save-dev babel-plugin-dynamic-import-node. 

Now, let's go ahead and update our Babel configuration to use this plugin, but we only want to use it when we're running our tests. We want Webpack to take over in production when we're building our application.

.babelrc.j:

    isTest ? 'babel-plugin-dynamic-import-node' : null,



Automatically setting up imports before each test.

If the same imports are being used across a number of test files we can ask Jest to import all these modules automatically. This saves a few import lines in each test file.

Throw all reused imports into setup-tests.js:

setup-tests.js

    import 'react-testing-library/cleanup-after-each'

    import {createSerializer} from 'jest-emotion'
    import * as emotion from 'emotion'

    expect.addSnapshotSerializer(createSerializer(emotion))


In jest.config.js there are a couple of available hooks:


module.exports = {
  testEnvironment: 'jest-environment-jsdom',
  moduleNameMapper: {
    '\\.module\\.css$': 'identity-obj-proxy',
    '\\.css$': require.resolve('./test/style-mock.js'),
  },
  // setupFiles: [],     // before Jest is loaded -  not so useful for us
  setupTestFrameworkScriptFile: require.resolve('./test/setup-tests.js'),  // after Jest is loaded
}



Custom Module Resolution:

Webpack’s resolve.modules configuration is a great way to make common application utilities easily accessible throughout your application. However, Jest does not use this configeration


The way our CalculatorDisplay is implemented, the calculator-display is imported. It's asynchronously loaded.

calculator.js:

    const CalculatorDisplay = loadable({
      loader: () => import('calculator-display').then(mod => mod.default),
      loading: () => <div style={{height: 120}}>Loading display...</div>,
    })

src/tests was like this:

test('renders', () => {
    const {container, debug} = render(<Calculator />)   // we see 'Loading Display...'
    debug(container)
})

But now we want to load it async so that the calculator display has time to display

    import loadable from 'react-loadable'

    test('renders', async () => {      // is no async with react-loadable imported
        await loadable.preloadAll()
        const {container, debug} = render(<Calculator />)  
        debug(container)
    }

However this will break with this error: Cannot find module 'calculator-display' from 'calculator.js'

The reason for this is because here, in calculator.js we're importing calculator-display as if it were a node module, but it's not a node module. It actually lives in the shared directory as calculator-display. And while webpack can resolve this module Jest is not configured to do so.

This is the webpack config:

    resolve: {
        modules: ['node_modules', path.join(__dirname, 'src'), 'shared']
    }

While we apply to jest.config.js

    const path = require('path')   // new import needed for moduleDirectories

    module.exports = {
      testEnvironment: 'jest-environment-jsdom',
      moduleDirectories: ['node_modules', path.join(__dirname, 'src'), 'shared'],
      moduleNameMapper: {
        '\\.module\\.css$': 'identity-obj-proxy',
        '\\.css$': require.resolve('./test/style-mock.js'),
      },
      setupTestFrameworkScriptFile: require.resolve('./test/setup-tests.js'),
    }




Support a Test Utilities File with Jest moduleDirectories

Every large testbase has common utilities that make testing easier to accomplish. Whether that be a custom render function or a way to generate random data. 

In this example, we now have a provider that supplies some styling based on theme. To make our test pass we need to remember to wrap up a component with the theme provider:

    import CalculatorDisplay from '../calculator-display'
    import {dark} from '../../themes'

    test('mounts', ( => {
      const {container} = render(
        <ThemeProvider theme={dark}>
          <CalculatorDisplay value="0" />
        </ThemeProvider>
      )
      expect(container.firstChild).toMatchSnapshot
    })

 This same thing would happen if we're using the Redux Provider, or React Router Provider, or some internationalization provider. It will be nice, if this render method could just render all the providers that we're really care about automatically for us. 
 
 Let's say we have a utils file that automatically returns a wrapped component:
 
test/calculator-test-utils.js:

    import React from 'react'
    import {render} from 'react-testing-library'
    import {ThemeProvider} from 'emotion-theming'
    import {dark} from '../src/themes'

    function renderWithProviders(ui, options) {
      return render(<ThemeProvider theme={dark}>{ui}</ThemeProvider>, options)
    }

    export * from 'react-testing-library'
    export {renderWithProviders as render}   // overwrite render
    
We also need to update module resolution in jest.config.js:

  moduleDirectories: [
    'node_modules',
    path.join(__dirname, 'src'),
    'shared',
    path.join(__dirname, 'test'),
  ],
       
       
 Now anywhere in our application, that we were using react-testing-library, we can actually use calculator-test-utils.
    
 IF we had some routers to wrap then our function might look like this:
 
     function renderWithProviders(ui, options) {
      render(
        <Router>
            <ReduxProvider store ={store || options.store}
              <ThemeProvider theme={dark}>{ui} </ThemeProvider> options
            </ReduxProvider>
      </Router>,
      )
    }
    
    
Note, you have to fix up eslint warning for module resoltion:
 
    $ npm install --save-dev eslint-import-resolver-jest
 
 and update jestconfig.js:
 
     overrides: [
        {
          files: ['**/__tests__/**'],
          settings: {
            'import/resolver': {
              jest: {
                jestConfigFile: path.join(__dirname, './jest.config.js'),
              },
            },
          },
        },
      ],




 Step through Code in Jest using the Node.js Debugger and Chrome DevTools
 
 Instead of console logs it would be nice to debug test code. First replace your console.log with a debugger keyword:
 
     getScale(){
        const node = this.node.current
        debugger;     //console.log('here')
 
Update package.json with a test:debug script 

    "scripts": {
        "test": "jest",
        "test:debug": "node --inspect-brk ./node_modules/jest/bin/jest.js --runInBand",
        // ...
}

When you run this script:

    $ npm run test debug
    
you will see a log:

    Debugger listening on ws://127.0.0.1:9229/2dbosnjnkj9-ajnkja- ....
    
Open Chrome and go to 

    chrome://inspect

And then click on inspect.

( If you do wnant to see source files look for an [sm] tag on the file name in the sources tab. )




Configure Jest to report code coverage on project files

Jest comes with code coverage reporting built-into the framework. To use it you need a --coverage flag in a script. A cover age folder is created, which should be added to gitignore.


package.json

  "scripts": {
    "test": "jest --coverage",
    
.gitignore
    node modules
    dist
    coverage
    
After running npm t we can open the coverage report:

    $ open coverage/lcov-report/index.html   // or just open the index.html file in your browser
    
If you click into individual files you see highlighted areas lacking test coverage. 

IF there are some folder that should not be included in test coverage we can exclude them (eg utilities files in the tests folder). In jest.config.js configuration add a property called collectCoverageFrom:

    collectCoverageFrom: ['**/src/**/*.js']    // only looks in the src folder, so will ignore the test folder
    
    
Coverage Reports

The code coverage tool uses Istanbul. It is possible to set a minimum global threshold and file specifc target in jest.config.js. 


  coverageThreshold: {
    global: {
        statements: 100,
        branches: 100,
        lines: 100,
        functions: 100,
    },
    './src/shared/utils.js': {               // file glob
      statements: 100,
      branches: 80,
      functions: 100,
      lines: 100,
    }
}
    }
    
Here we see that the util.js file is of less importance; there is some branch that has low priorty for a test.

Not reaching these targets causes a test failure.

Codecov is a CI tool can help you track code coverage over time.

You can make jest work in a watch mode. Inpackage.json scripts add in 

    "test:watch": "jest --watch",

Then run $ npm run test:watch.

There are many options like pattern mode, all mode, and changes since last commit. Press 'w' for more options. The t key will filter on the test name rather than the file name. We can be hyper focused just on a failing test. This is the same as a test.only(), but without having to change any code.


In CI, we don’t want to start the tests in watch mode, but locally we normally want to run the tests in watch mode. We can have separate scripts, but it’d be great to not have to remember which script to run locally. Let’s use is-ci-cli to run the right script in the right environment when running the test script.

    $ npm install --save-dev is-ci-cli
    
 
"scripts": {
    "test": "is-ci \"test:coverage\" \"test:watch\"",    // like an OR
    "test:coverage": "jest --coverage",
    "test:watch": "jest --watch",


Jest’s watch mode is pluggable and jest-watch-typeahead is one plugin that you definitely don’t want to live without. It enhances the watch mode experience to help you know which tests will be run based on your filter. It scans a head and lists out the test that will match your pattern before you have run the tests. You can even arrow up and down through the choices.

    $ npm install --save-dev jest-watch-typeahead
    
jest.config.js:

    watchPlugins: [
        'jest-watch-typeahead/filename',
        'jest-watch-typeahead/testname',
    ]  

Sometimes you may have situations where configuration needs to be different for certain tests.  We’ll take a look at how we could create a custom configuration for tests that are intended to run in a node environment, say on the server side.

The first step is to create a jest config file for each situation. And eacg of these will require teh main shared config file. For example the client side tests might be configured in jest.client.js:

module.exports = {
  ...require('./jest-common'),                                   // Base config. Everything else is an override.
  testEnvironment: 'jest-environment-jsdom',                       // browser like environment
  setupTestFrameworkScriptFile: require.resolve('./test/setup-tests.js'),
  coverageThreshold: {
    global: {
      statements: 17,
      branches: 4,
      functions: 20,
      lines: 17,
    },
    './src/shared/utils.js': {
      statements: 100,
      branches: 80,
      functions: 100,
      lines: 100,
    },
  },
}

For server tests, we could have jest.server.js

const path = require('path')

module.exports = {
  ...require('./jest-common'),
  testEnvironment: 'jest-environment-node',
  testMatch: ['**/__server_tests__/**/*.js'],
  coverageDirectory: path.join(__dirname, '../coverage/server'),  // change the coverage dir so we odn't override client test coverage
}


To find your tests spread around your project, you may need to specify a rootDir property in your config

    rootDir: path.join(__dirname, '..'),
 
package.json
"scripts": {
    "test": "is-ci \"test:coverage\" \"test:watch:client\"",
    "test:coverage": "npm run test:coverage:client && npm run test:coverage:server",
    "test:coverage:client": "jest --config test/jest.client.js --coverage",
    "test:coverage:server": "jest --config test/jest.server.js --coverage",
    "test:watch:client": "jest --config test/jest.client.js --watch",
    "test:watch:server": "jest --config test/jest.server.js --watch",
    "test:debug:client": "node --inspect-brk ./node_modules/jest/bin/jest.js --config test/jest.client.js --runInBand --watch",
    "test:debug:server": "node --inspect-brk ./node_modules/jest/bin/jest.js --config test/jest.server.js --runInBand --watch",
    
    
You may also have to update .eslintrc.js with the new jestConfigFile location.

Now, however, we have a number of commends. We can use Jest's projects feature to have jest run both configurations at once. 

In the command line it looks like this:

    $ npx jest --projects ./test/jest.client.js ./test/jest.server.js

And if we want combined code coverage accross both configurations then:

    $ npx jest --projects ./test/jest.client.js ./test/jest.server.js --coverage   // could also tack on --watch
    
We can configure this in jest.config.js (at the root):

    ...require('./test/jest-common'),
    projects: ['./test/jest.client.js', './test/jest.server.js'],
    collectCoverageFrom: ['**/src/**/*.js'],
    // put any other global conif options here too: see applied config with $ npx jest --showConfig --config ./test/jest.client.js)    

This will create a shared coverage report.

Now that both areas are combined we can imprive the logging in the console with a display name property:

jest.server.js
    displayName: 'server',

jest.client.js
    displayName: 'dom',
    
 
Now package.json is simplified via the default configurations:

"scripts": {
    "test": "is-ci \"test:coverage\" \"test:watch\"",
    "test:coverage": "jest --coverage",
    "test:watch": "jest --watch",
    "test:debug": "node --inspect-brk ./node_modules/jest/bin/jest.js --runInBand --watch",
    
If anybody ever wants to run a single configuration, they can just run 
    $npx jest --config test/jest.client.js
  
One last thing that we can do here is update our .eslintrc.js to go back to our jest.config.js.

    jestConfigFile: path.join(__dirname, './jest.config.js'),  
  
You can also create Jest console options (navigaable with the arrow keys) to toggle between options. Update watchPluggins:

    $ npm install --save-dev jest-watch-select-projects

jest.common.js
    watchPlugins: [
        'jest-watch-typeahead/filename',
        'jest-watch-typeahead/testname',
        'jest-watch-select-projects',
    ]
    
 Now when you run $ npm t you'll get an extra capital 'P' option:
 
 Watch Usage
 › Press a to run all tests.
 › Press f to run only failed tests.
 › Press q to quit watch mode.
 › Press p to filter by a filename regex pattern.
 › Press P to select projects (all selected).
 › Press t to filter by a test name regex pattern.
 › Press Enter to trigger a test run.
 
 

Run ESLint with Jest using jest-runner-eslint

Jest is more than a testing platform It is excellent at running tasks in parralel and it has a capability to run more than just tests and. We can specify a custom runner, and watch mode can be used elsewhere. 

You can create runners fro many things, such as python and Go tests. Here we extend Jest's features to linting.

    $ npm install --save-dev jest-runner-eslint
    
jest.lint.js
    const {rootDir} = require('./jest-common')  
    
    module.exports = {
        rootDir,
        runner: 'jest-runner-eslint',
        displayName: 'lint',
        testMatch: ['<rootDir>/**/*.js'],
        testPathIgnorePatterns: ['/node_modules/', '/coverage/', '/dist/', '/other/'],
        
    $ npx jest --config test/jest.lint.js
  
So now we can  use watch mode and see the linting workthough each file as if it was a test.

We could also update projects with our new config, so that npm t will also lint:

jest.config.js
    projects: [
    './test/jest.lint.js',
    './test/jest.client.js',
    './test/jest.server.js',
    ],
  
package.json
    "lint": "jest --config test/jest.lint.js",
        "format": "prettier \"**/*.js\" --write",
        "validate": "npm run test && npm run build",        
        
        
        
 Run only relevant Jest tests on git commit
 
Running the project tests on commit is a great way to avoid breaking the application accidentally and leverage the mechanism for confidence you have from your testbase. However, as the testbase grows this can take a very long time and slow productivity down. Let’s see how Jest is capable of running only the tests and linting only the files that are affected by the files we’re committing.

Jest has a flag to run only tests related to a file:

    $ npx jest --findRelatedTests src/shared/auto-scaling-text.js
    
To extend this into commits:

    $ npm install --save-dev lint-staged husky
    
Husky will be responsible for installing git hooks that we can configure in our package.json which is done right here.    

Now open our package.json here and configure a pre-commit script, which husky will read and run before git commits any code. Here, we can use lint-staged, which will run some scripts based off of some configuration that we provide.

package.json
    "precommit": "lint-staged",
    
lint-staged.config.js

    module.exports = {
    linters: {
        '**/*.js': ['jest --findRelatedTests'],
    },
    } 
 
 This is going to run husky, which is going to run lint-staged, which will run Jest find related tests for each one of these files. 
 
 
 
 
 
 Cypress
 ----------------------------------------------------
 
    $ npm install --save-dev cypress   // yarn add cypress --save
 
 .eslintrc.js
    plugins: ['eslint-plugin-cypress'],
    env: {'cypress/globals': true},
 
 .gitignore:
    cypress/videos
    cypress/screenshots
 
Then start cypress: $ yarn run cypress open
 
In the tool that opens up you can click on settings to see the config and environment variables that are available.
 
 
Example:

counter.js (in the cypress directory):

describe('Counter', ()=> {
    it('can increment', () => {

        const user = cy;  // by calling cy `user` you better represent the user focus
        user                                  // chain async commands:
            .visit('http://localhost:3001/') //go to server address
            .get(':nth-child(3) > :nth-child(3)')   // change subjct to button (use selection tool)
            .click()                // click actioned on most recent subject
            .get(':nth-child(3) > div')   // change subject again
            .should('have.text', '1')  // make assertion on expected UI changes
    })
})

After running the test you can click on the history on the LHS to see each step as a visual snapshot.
 
On minor problem is that supplying a full url in visit() causes a refresh on each test. Instead we can store the URL in the config and tell visit to go to root (i.e.  .visit('/') ).


cypress.json

{
    "baseUrl": "http://localhost:3001/"
}

The default folder name of 'integration' is not idea. Rename it to e2e and update the config

e2e/cypress.json:

{
  "baseUrl": "http://localhost:8080",
  "integrationFolder": "cypress/e2e"
}

You can also configure the viewport:
    "viewportHeight": 900,
    "viewportWidth": 400

There's also an env file which you can use to configure Cypress. You can use environment variables, CLI arguments, when we open up Cypress in the first place, as well as a plugin file. 

 We can also improve the selectors so that they are more readable:
 
    $ npm install --save-dev @testing-library/cypress  // get dome test library into cypress
 
In support/index.js ` import '@testing-library/cypress/add-commands'`. This is going to add custom commands to Cypress that we can use in our tests. In this case, we can now use getByText. 

    .findByText(/^+$//) // enter regex that matches the text in the '+' button

And test ids are also available:

    .findByTestId('count')   // change subject again




Node Scripts

We can figure a script to start the server, and then wait untill the serer is reposding to open cypress. First install:

    $ npm install --save-dev start-server-and-test
    
"scripts": {
    "cy:run": "cypress run",
    "cy:open": "cypress open",
    "test:e2e": "is-ci \"test:e2e:run\" \"test:e2e:dev\"",
    "pretest:e2e:run": "npm run build",   // make sure the lates changes have built before testing ('pre' is an automatic hook)
    "test:e2e:run": "start-server-and-test start http://localhost:3001/ cy:run"  // wait for server to respond and then runs cypress
    "test:e2e:dev": "start-server-and-test dev http://localhost:8080 cy:open",
    "validate": "npm run test:coverage && npm run test:e2e:run",
}
   
   
   
Debugging
 
Cypress will pop up the dev tools if you have a standard debugger statement. However, to place that keyword in a useful location you need to insert a then() into the chain. It callback simplly needs to return what ever it receives so that the chian can continue.   
    
    cy.visit('/')
      .getByText(/^1$/)
      .click()
      .getByText(/^\+$/)
      .click()
      .getByText(/^2$/)
      .then(subject => {       // here
          debugger             
          return subject
      })
      .click()
      .getByText(/^=$/)
      .click()
      .getByTestId('total')
      .should('have.text', '4')


You can also 'debuger' in your source code. However, so that you can access variables atfter a test has run you can test for cypress and then add information to the window. Here is some component and we want to access the state at one point of time, but later after running the test.

ReactDOM.render(
  <Component initialState={{}}>
    {({state, setState}) => {
      if(window.Cypress) {                         
        window.appState = state
        window.setAppState = setState
      }
    //...

Now you can check out window.appState in the dev tools console.



Backend/user Registration test example

generate.ts  // will generate unique user data each time - you might also have to clear the database for each test run and mock any email reponses

import {build, fake} from 'test-data-bot'

const userBuilder = build('User').fields({
  username: fake(f => f.internet.userName()),
  password: fake(f => f.internet.password()),
})

export {userBuilder}

register.js (test file)

import {userBuilder} from '../support/generate'

describe('registration', () => {
  it('should register a new user', () => {
    const user = userBuilder()
       cy.visit('/')
      .getByText(/register/i)
      .click()
      .getByLabelText(/username/i)
      .type(user.username)
      .getByLabelText(/password/i)
      .type(user.password)
      .getByText(/submit/i)
      .click()
      .url()
      .should('eq', `http://localhost:8080`)
  })
})



Test/Cypress driven Development

Because you have access to the dev tools in the cypress tools you can use Cypress in TDD mode. You do not a separate browser window.


Simulate HTTP Errors

While it is normally preferable to test error states using integration or unit tests, there are some situations where it can be really useful to mock out a response to test a specific scenario in an E2E test. Let’s use the cypress server and route commands to mock a response from our registration request to test the error state.

it.only(`should show an error message if there's an error registering`, () => {
  cy.server()       // macks a fake cypress server to intercept requests
  cy.route({          
      method: 'POST',
      url: 'http://localhost:3000/register',
      status: 500,                              // send back an error code
      response: {},                         // could speicify a reponse if required
   })
  cy.visit('/register')             // try to register with no details in the input fields
    .getByText(/submit/i)
    .click()
    .getByText(/error.*try again/i)       // should get error text
})

As tests grow we run the risk of adding tests that do little to imprive confidence and yet they add to the noise if there is a bug. For example one test may test registration. But then another test, which tests logging back in, may need to run through the registration steps as part of it's set up phase. We already have a test for registration and so we don't want this second test to fail if registration fails. In the second test one work around is bypass the UI-user focus involved in registration and just use the API: 

    cy.request({
        url: 'http://localhost:3000/register',
        method: 'POST',
        body: user,
    })
 
 Now, since we might often like to register new users, we can set up a custom command.
 
 support/commands.js:
 
 import {userBuilder} from './generate'

    Cypress.Commands.add('createUser', overrides => {
        const user = userBuilder(overrides)
        cy.request({
        url: 'http://localhost:3000/register',
        method: 'POST',
        body: user,
        })
    })
 
  'oversides' allows for optional parameters to be passed in.
  
  So, you could paste this into your test now:
  
  cy.createUser()
    cy.visit('/')
      .getByText(/login/i)
       .click()
      .getByLabelText(/username/i)
      .type(user.username)           // error here

But now the user object is not defined. However, cy.request returns the response objet, which will contain user data. If we attached a .then() to the custom command, we can then wrap the rest of the test code:


command.js:

import {userBuilder} from './generate'

Cypress.Commands.add('createUser', overrides => {
  const user = userBuilder(overrides)
  return cy
    .request({
      url: 'http://localhost:3000/register',
      method: 'POST',
      body: user,
    })
    .then(({body}) => body.user)    // response body contains the user
})

(you can get the method and URL from inspecting the dev tools. Click on the relevant step in the cypress tools history and you should het to the request)

test:

describe('login', () => {
  it('should login an existing user', () => {
    cy.createUser().then(user => {
      cy.visit('/')
        .getByText(/login/i)
        .click()
        .getByLabelText(/username/i)
        .type(user.username)       // all good now.
        // ....


Here is another example that uses a custom user registration but then also logs in the user. This is a good set up for all tests that assume a user has bene logged in already. Doing it this way will also speed up your tests.

    cy.createUser().then(user => {
      // login as the new user
      cy.request({
        url: 'http://localhost:3000/login',
        method: 'POST',
        body: user,
      }).then(({body}) => {
        window.localStorage.setItem('token', body.user.token)    // inspecting the reponse shows that a returned token is set in localstorage
      })


We can also use custom cypress commands for reusable assertions. When we have the same assertion used in more than one location we can both absract-awaythis duplicate code and also make a our tests more readable.

Say some tests have these should() assertions:

describe('registration', () => {
  it('should register a new user', () => {
       // ...
    .url()
    .should('eq', `${Cypress.config().baseUrl}/`)
    .window()
    .its('localStorage.token')
    .should('be.a', 'string')
    .getByTestId('username-display')
    .should('have.text', user.username)

In commands.js we can break up these two assertions:

Cypress.Commands.add('assertHome', () => {
  cy.url().should('eq', `${Cypress.config().baseUrl}/`)
})

Cypress.Commands.add('assertLoggedInAs', user => {
  cy.window()
    .its('localStorage.token')
    .should('be.a', 'string')
    .getByTestId('username-display', {timeout: 500})
    .should('have.text', user.username)
})

Now our test, and other that use these asserts look cleaner:

describe('registration', () => {
  it('should register a new user', () => {
       // ...
      .assertHome()
      .assertLoggedInAs(user)
  })

If you are testing that something does not exist getByText will not work. After 4000ms it throws an error if an element does not exist. Instead use queryByTestId. Eg, say we are testing for a log out then the user name display should no longer be there. 

        .getByText(/logout/i)                
        .click()                                            //click the logout button
        .queryByTestId('username-display', {timeout: 300})  // 300 ms timout to wait for the user display to exist
        .should('not.exist')



