

Fundamentals of Testing In Javascript
----------------------------------------------

Say We have a bug in our sum function. To make sure it enver happens again we write a test. Essentially, . A test is code that throws an error when the actual result of something does not match the expected output. 

    const sum = (a, b) => a - b;

    const result = sum(3, 7);
    const expected = 10;
    if (result !== expected){
        throw new Error(`${result} is not equal to ${expected}`)
    }

The  job of a Javascript testing framework is to make that error message as useful as possible so we can quickly identify what the problem is and fix it.

To make this code less imperative, and more declarative, we can create an assertion library.

    function expect(actual) {
      return {
        toBe(expected) {
          if (actual !== expected) {
            throw new Error(`${actual} is not equal to ${expected}`)
          }
        }
      }
    }

    const result = sum(3, 7);
    const expected = 10;
    expect(result).toBe(expected);

We can add other assertions to the library object. Eg:

    toEqual(expected) {}
    toBeGreaterThan(expected) {}
    
One problem with this approach is that as soon as one test fails, and throws an error, no other tests will be run. When finding the cause of a test failure, knowing the state of other tests can be useful. In fact even some clear logging around which test actually failed is important.    
    
    function test(title, callback) {
      try {
        callback()
        console.log(`✓ ${title}`)
      } catch (error) {
        console.error(`✕ ${title}`)
        console.error(error)
      }
    }
    
    test('sum adds numbers', () => {
      const result = sum(3, 7)
      const expected = 10
      expect(result).toBe(expected)
    })
    
A test library should quickly identify what is broken so that we can make a quick fix.

The next step is to handle async code. Even if we make our callback async the test function will complete and then later an unhandled promise rejection error will occur along with the failed test info.

    test('sumAsync adds numbers asynchronously', async () => {
      const result = await sumAsync(3, 7)
      const expected = 10
      expect(result).toBe(expected)
    })

    > (node:11815) UnhandledPromiseRejectionWarning: Unhandled promise rejection (rejection id: 1) ... Error: -4 is not equal to 10 ...
    
The answer is to add an await to our test function. We need to wait for the callback to finish:

    async function test(title, callback) {
      try {
        await callback()
        console.log(`✓ ${title}`)
      } catch (error) {
        console.error(`✕ ${title}`)
        console.error(error)
      }
    }
    
 Many test libraries are set up as global variables. We can do that my moving our library into another file, setup-global.js. We add some exports at the bottom of this file and then we can run our tests:
 
    // bottom of setup-global.js
    global.test = test
    global.expect = expect
    
    > node --require ./setup-globals.js lessons/globals.js
 
 
 
Static Analysis Testing
--------------------------------------------

Static analysis involves no dynamic execution of the software under test and can detect possible defects in an early stage, before running the program. Static analysis can be done by a machine to automatically “walk through” the source code and detect noncomplying rules. Static analysis can also be performed by a person who would review the code to ensure proper coding standards and conventions are used to construct the program. This is often called Code Review.

Quality attributes that can be the focus of static analysis:
 - Reliability
 - Maintainability  (also inlcludes comments)
 - Testability  (are there tests?)
 - Re-usability
 - Portability
 - Efficiency
 
Once tool is eslint. You can set up rules in .eslintrc

    {
      "parserOptions": {
        "ecmaVersion": "2018"
      },
       "extends": [
        "eslint:recommended"    // automatically use the reccomended set of rules
      ],
      "rules": {
        "valid-typeof": "error" // will break the build. Can use "warn" instead. Or, can use "off" to disable the rule.
      }
    }

So you don't have to run $ npm eslint src all the time you can add eslint to your package.json:

    "scripts": {
        "lint": "eslint src"
    },
    
You can also just add an eslint extension to your code editor. Since you don't have to run anything you get a faster feedback loop.


Code formatting tools, like perttier are also available.Such tools limit arguments with others and tidy the code.

    $ npx prettier src/example.js  // just see the result
    $ npx prettier --write src/example.js  // write the result to the file, or:
    
    "scripts": {
        "lint": "eslint src",
        "format": "prettier --write \"**/*.+(js|jsx|json|yml|yaml|css|less|scss|ts|tsx|md|mdx|graphql|vue)\""
    },
    
There may also be some code editor support for prettier. If so you should enable format on save in the editor's settings. You can also change prettier's setting with a .prettierrc file. Go to https://prettier.io/playground/ to try out and generate some config options. A .prettierignore can be used to ignore some other file types like node modules.

If there are conflicts between prettier and eslint then try this module: npm install --save-dev eslint-config-prettier, cnad update extends in .eslintrc:

  "extends": [
    "eslint:recommended", "eslint-config-prettier"
  ],
  
 [this example conintinues into the node scripts section of the node training file]
 
 
 
 Mocking
 ------------------------------------------------
When running unit tests, you don’t want to actually make network requests or charge real credit cards. This can easily get slow and flaky. Instead we want some aspects of the code such as inputs into a function under test to be deterministic. 

One way to do this is 'monkey-patching'. We import some functionality but assign some deterministic replacement. Then we properly dispose of that replacement.

    const originalGetWinner = utils.getWinner    // we will overwrite getWinner, which returns a random result used within thumbWar()
    utils.getWinner = (p1, p2) => p1            //  p1 will always be the return value used within thumbWar().

    const winner = thumbWar('Kent C. Dodds', 'Ken Wheeler')
    assert.strictEqual(winner, 'Kent C. Dodds')    // if the rest of thumbWar is working then p1 should now always be the result

    utils.getWinner = originalGetWinner   // properly dispose of our version of getWinner i.e. fix up the original object.
    

In this case it would also be great to test how getWinner is being called. For example, what if it was not called with the correct arguments. Say only p1 and not p2. The test would still pass but the implementation would be wrong.
        
     test('returns winner', () => {
      const originalGetWinner = utils.getWinner
      utils.getWinner = jest.fn((p1, p2) => p1)

      const winner = thumbWar('Kent C. Dodds', 'Ken Wheeler')
      expect(winner).toBe('Kent C. Dodds)
      expect(utils.getWinner).toHaveBeenCalledTimes(2)
      expect(utils.getWinner).toHaveBeenCalledWith('Kent C. Dodds', 'Ken Wheeler')  // picks up the implementation break
    })


Or if we expect the mocked function to have been called with different arguments:

  expect(utils.getWinner).toHaveBeenNthCalledWith(1, 'Kent C. Dodds', 'Ken Wheeler')  // arguments for first call - '1'
  expect(utils.getWinner).toHaveBeenNthCalledWith(2, 'Kent C. Dodds', 'Henry Jones')
  
Inspecting the mocked function object actually allows us to replace many of these expectations. For example, you can log it out and see the calls:

    console.log(utils.getWinner.mock.calls); // see and array of 'calls'
    
 Now all we need to test is this:
 
     expect(utils.getWinner.mock.calls).toEqual([
      [ 'Kent C. Dodds', 'Ken Wheeler' ],
      [ 'Kent C. Dodds', 'Ken Wheeler' ]
    ]);
    
That is, we know there have been two calls and what the arguments for those calls have been, with just one assertion.


We could make our own mocking function to get the same result:

    function fn(impl) {
      const mockFn = (...args) => {
        mockFn.mock.calls.push(args)
        return impl(...args)
      }
      mockFn.mock = {calls: []}
      return mockFn
    }
    
    const originalGetWinner = utils.getWinner
    utils.getWinner = fn((p1, p2) => p1)

    assert.strictEqual(winner, 'Kent C. Dodds')
    assert.deepStrictEqual(utils.getWinner.mock.calls, [
      [ 'Kent C. Dodds', 'Ken Wheeler' ],
      [ 'Kent C. Dodds', 'Ken Wheeler' ]
    ])
        
This fn function accepts an implementation and returns a function that calls that implementation with all of those arguments.

It also keeps track of all the arguments that it's called with so that we can assert how that function is called, allowing us to catch issues in our integration with the getWinner function.
 
Having to dispose of our version of the mocked function can be tiresome. The workaround is to use spyOn and restore. spyOn takes an object and a method:

    test('returns winner', () => {
      jest.spyOn(utils, 'getWinner')   // instead of `const originalGetWinner = utils.getWinner`
      utils.getWinner.mockImplementation((p1, p2) => p2) // replaces utils.getWinner = jest.fn((p1, p2) => p2)

      ...

      // cleanup
      utils.getWinner.mockRestore()  // instead of `utils.getWinner = originalGetWinner`
    
    
 To make our own version we first need to update our mock function factory with a mockImplementation method:
 
    function fn(impl = () => {}) {
      const mockFn = (...args) => {
        mockFn.mock.calls.push(args)
        return impl(...args)
      }
      mockFn.mock = {calls: []}
      mockFn.mockImplementation = newImpl => (impl = newImpl)  // this allows our tests to be deterministic
      return mockFn
    }

And spyOn:

    function spyOn(obj, prop) {
      const originalValue = obj[prop]
      obj[prop] = fn()
      obj[prop].mockRestore = () => (obj[prop] = originalValue)
    }
    
So, SpyOn is resonisble for tracking the original value and then restoring the originalValue.


So far our monkey-patching has been working because we have been using common JS. It does not work with es6 modules. However, jest allows you to mock an entire module with the jest.mock API. The first argument is the module being mocked, realtive to the call to jest.mock. The 2nd argument is a module factory function that will return a mocked version of the module.

    jest.mock('../utils', () => {
      return {
        getWinner: jest.fn((p1, p2) => p1)
      }
    })

    test('returns winner', () => {
        // remove spyon and mock implementation
      const winner = thumbWar('Kent C. Dodds', 'Ken Wheeler')
      expect(winner).toBe('Kent C. Dodds')
      expect(utilsMock.getWinner.mock.calls).toEqual([
        ['Kent C. Dodds', 'Ken Wheeler'],
        ['Kent C. Dodds', 'Ken Wheeler']
      ])

      // cleanup
      utils.getWinner.mockReset() // replace mockRestore with mockReset
    })
    
Also before Jest runs our code, it moves the jest.mock call up to the top of the file to ensure that the mock is in place before any of our modules are loaded. This is useful as other imports are often hoisted to the top of the file.
    
If you wanted to implement this yourself you would need the require cache. If you were to log it out you would see an object with many keys:

    console.log(require.cache);
    
    { '/Users/kdodds/Developer/js-mocking-fundamentals/node_modules/jest-worker/build/child.js':
      Module {
        id: '.',
        exports: {},
        parent: null,
        ...
      }
    }
    
Let's go ahead and get the utilsPath with require.resolve('../utils'). We'll then say require.cache at the utilsPath equals an object, and this object needs to resemble a module, so we'll say id is utilsPath, the filename is utilsPath, loaded is true, and exports is our mock, so we'll say getWinner is a call to our function with p1, p2 always returning p1. 
    
    const utilsPath = require.resolve('../utils')
    require.cache[utilsPath] = {
      id: utilsPath,
      filename: utilsPath,
      loaded: true,
      exports: {
        getWinner: fn((p1, p2) => p1)
      }
      
      // cleanup
       delete require.cache[utilsPath]
    }
 
With that, we can change things here a little bit. Let's put fn() up here at the top. We'll get rid of this spyOn function, we don't need that anymore. We'll get rid of our getWinner.mockImplementation call here.

We'll change our cleanup to delete require.cache at that utilsPath. That way any other modules that want to use the utils can do so without having trouble with our module mocking them out.

We could also preload the require.cache with the mock module that we want to have loaded when thumb-war requires the utils module.

    require.cache[utilsPath] = {
      id: utilsPath,
      filename: utilsPath,
      loaded: true,
      exports: {
        getWinner: fn((p1, p2) => p1)
      }
    }


If more than one test will need a mock of a module you can externalise your mock directory. Create a __mocks__ directory next to the file you wish to mock. Inside this directory create a file with the same name as the file to be mocked. 

For example, if we are to mock the getWinner method from the utils directory, the __mocks__/utils will look like:

    module.exports = {
      getWinner: jest.fn((p1, p2) => p1)
    }

In our test file we now need this:

    jest.mock('../utils')

