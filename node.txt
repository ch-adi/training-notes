

******************************************

                Node

******************************************

Node is a JS runtime environment. It is not the JS language, but rather allows you to run JS on a server. Browsers on the other hand allow you to run JS on the client.  Both Chrome and Node create these runtimes by using the V8 JS engine. This is written in c++, and so is most of Chrome and Node. This allows JS to do anything c++ can do, say like access file systems. Chrome and Node, however provide different runtime features wrapped around the V8 engine. That is they provide differnet 'bindings' that enhance the language. For example, in a browser you  have a window and the DOM, and so the global object is `window` and that also contains a 'document` object. These are not in the language specs, but rather part of the runtime. v8 doesn't know about the DOM, but the browser will connect everything together. Instead wondow and document objects, Node supplies a `global` object and a `process`, which makes more sense on the server side.

Node uses an event driven, non-blocking I/O model. External requests don't block.

Javascript is quite a high level language. Functionality like memory menagement is abstracted away. Also, because js was created for use in browsers it lacks some other common functionality such as modules and sending files around. V8 is an open source c++ applicaiton that converts JS into machine code instructions for a variety of common microprocessor arcitectures. It is thus a JS engine, and it also meets all the ECMA script specifications for a JS engine. The advantage of v8 is that it has hooks so that it can be embedded in other c++ applications. Node is such an application. It takes v8 and extends it so that JS can be given extra functionality. This allows JS to be used outside of the browser. In particular it gives JS enough functionality to be used on a server (the client-server model is common, and the best example is on the internet, which uses http to communicate between clients and servers). A server is connected to the internet and offers services.

Some features missing from plan js that are addressed in node:

- better way to organise code into reusable pieces - ie modules
- dealing with files - streams and buffers
- databases
- communicate over the internet - node can deal with TCP/IP requests 
- accept requests and send responses - messages are built into node and use HTTP
- deal with work that takes a long time - async methods and event loops. Streams for performance


Running Node in the Command Line
----------------------------------

$ node app.js [arguments]

Any arguments that you pass in are attached to process.argv, which is the "argument vector". The first two members of this vector are always the path to the node executable, and the second is the path to the file, which is app.js here.

$ node app.js add --title-"first note"

[
  'C:\\Program Files\\nodejs\\node.exe',
  'C:\\Users\\cliff\\Documents\\Cliff\\repos\\rtl\\node-course\\notes-app\\app.js',       
  'add',
  '--title=first note'
]

A good package to parse these strings is yargs. In addition to producing a more useful arguments object, it will also enable you pass in --help and --version when running your app.js. It will automatically find the relevant content for the console.

Say we want four commands in our notes app: add, remove read, list. Here we add the 'add' command:


app.js
yargs.command({
    command: 'add',
    describe: 'Add a new note',   // --help description
    builder: {
        title: {
            describe: 'Note title',
            demandOption: true,  // must have a title argument
            type: 'string'  // title must be of type string
        },
        body: {
            describe: 'Note body',
            demandOption: true,
            type: 'string'
        }
    },
    handler: function(argv) {
        console.log('adding a new note', argv)
    }
});

yargs.command({
    command: 'remove',
    describe: 'Remove a note',
    handler: function() {
        console.log('Removing a note')
    }
});


  yargs.parse(); // needed to run the above command
  console.log(yargs.argv)
  
  
$ node app.js --help
  
  app.js [command]

  Commands:
    app.js add  Add a new note

  Options:
    --help     Show help  
    --version  Show version number  

$ node app.js add --title="first note" --body="first note body"

  adding a new note {
    _: [ 'add' ],
    title: 'first note',
    body: 'first note body',
    '$0': 'app.js'
  }
  
  
  






Modules 
----------------------------------

A module is a reusable block of code whose existence does not accidentially impact other code. Javascript did not orignally have this feature (although ECMAscript 6 spec does require it now). Node does this with 'CommonJS modules'.

CommonJS modules are an agreed upon standard for how code modules should be structured. 

Node supplies a built in function for importing modules called 'require('./filePath')'. 

This will make any code that is invoked within that required file run. However any functions from that file can't just be called. This is to prevent naming collisions.

So to call functions from the required file we have to use 'module.exports' in the required file. 

greet.js:

    // this line runs when greet.js is 'required' in app.js. You don't even need to assign require to a variable
    console.log('Due to require(), this appears automatically in app.js');

    // this will not run in app.js unless we assign it to module.exports.
    var greet = function() {
        console.log("hello");
    };

    module.exports = greet;

app.js

    // to just get that first line from greet running (ie requiring a module means that all it's code will run)
    // require('./greet');

    // now we use the idea that the require function returns 
    // module.exports.
    const greet = require('./greet');

    greet();

To explain how modules work in nodejs we first have to look at scope.

Scope: where in code you have access to a particular variable or function. 

Scope example with an IIFE, that shows how JS devs once faked up modules:

    var firstName = 'Jane';

    // the () around the function turns it into an expression that the engine
    // then holds and waits to see what you do with it. We execute it.
    
    (function () {
        var firstName = 'John';
        console.log(firstName);
    })();

    console.log(firstName);

    // john
    // Jane

So firstName in the function is protected from firstName from the global scope.

`require` is a function that takes a path. When you require a module in node, node wraps the code in your module within an IIFE. This protects the variable in a new scope, which is then passed into the v8 engine. module.exports is then returned.

If require() cannot find a file that matches it's string argument it then looks for a folder of the same name. Then it will look for an index.js file within that folder. So you can require a number of files in the index.js, and include their return values in one module.exports object. Then, from say app.js, just require the folder that index.js lives in. You can run functions from any of the original files from a single object. 

If `require()` is given the path of a JSON file (wich is really just a text file in JSON format - no methods and property names/keys in quotes), then that JSON string is returned back as a JSON object. 

Common Module Patterns


When node begins parses through a module, module.exports is first created as an empty object. So if you have just a function you are returning then just assign the function definition right to module.exports :

greet.js:

    module.exports = function() {
        console.log('hello');
    }

app.js:

    var greet = require('./greet');
    greet();


Also, because require is returning back our module.export as an object, often people use dot notation.

app.js:

    var greet2 = require('./greet').greet;
    greet2();


We can also employ other ways to make objects such as a function constructor (see refresher notes at end)

greet.js:

    function Greetr() {
        this.greeting = 'Hello World!';
        this.greet = function() {
            console.log(this.greeting);
        }
    }

    // replace the empty object with our own object
    module.exports = new Greetr();

app.js:

    const greet = require('./greet');
    greet.greet();


However, we have to be careful when generating what looks like two different objects:

    const greet1 = require('./greet');
    greet1.greeting = 'Changed Hello World'

    const greet2 = require('./greet');
    greet2.greet();  // Changed Hello World


Here, eventhough it looked like we 'newed up' two different objects, the variables actually hold a reference to the same object. `require` actually caches a return object for each file name.

An alternative, and 4th pattern, is to just place a reference to the constructor function in module.exports and let the `new` happen in app.js:

    module.exports = Greetr;

app .js:

    const Greetr = require('./greet');
    const greet1 = new Greetr();
    greet1.greeting = 'Changed Hello World';

    const greet2 = new Greetr();
    greet2.greet();  // 'Hello World!


The next pattern is very popular and useful:

greet.js:

    var greeting = 'Hello world!';

    function greet() {
        console.log(greeting);
    }

    module.exports = {
        greet: greet
    }


app.js:

    const greet = require('./greet').greet;
    greet();  // 'Hello world!'

So we no longer have access to the greeting variable in greet.js, but we can still see it being applied. We essentialy have a private variable. This is called the 'revealing module pattern': exposing only the properties and methods you want via a returned object. This is a common and clean way to structure and protect code within modules.


Exports vs module.exports.

In your module you can actually use 'exports' instead of module.exports (exports is the parameter name and module.exports is the argument for the IIFE that is wrapped around the code in your module). Both variables point ot the same object in memory. However, exports is tricky to use. If you use an assignment at the top level of the object then you are creating a new object and thus a new reference. It's a lot easier to never use it.



Requiring Native (Core) Modules

Native modules come from the lib folder in node. They are the core of the JS side. You can see all the native modules in the node API. Some need to be explicitly imported, ie `required1 and some do not. You can look in lib to see the file name and work out the path string you will need. However, you don't need a full file path. Just the filename will do. Eg for the Utilities module:

    var util = require('util'); // for libs/util.js

If you happen to have one of your own modules with the same name then using the path separator will fix the problem: require('/util'). But this is not recommend anyway since it may lead to confusion.

Some of these modules are wrappers for c++ code and some are just js that you could have written yourself (but why bother?). Here we use the utilities module to date stamp, format and log a variable:

    const util = require('util');
    var name = 'Tony';
    var greeting = util.format('Hello, %s', name);
    util.log(greeting); // 13 Dec 15:28:09 - Hello, Tony 



Modules and ES6

ES6 modules are also suported in v8 (but not node). Eg:

greet.js:

    export function greet() {
        console.log('Hello');
    }

app.js:

    import * as greetr from 'greet';
    greetr.greet()




Saving and reading JSON to/from file
---------------------------------------





Events and the Event Emitter
---------------------------------------

Many core node js modules are built upon this concept.

Events in nodejs are:

    'Something that has happened in our app that we can repsond to'.

In nodejs we actually talk about two different kinds of events. On one side we have system events. They come from the c++ side of the nodejs core thanks to a library called libuv. These are events that have come from the computer system, like "I've finished reading a file", or "I've recieved information from the internet". These are events that JS did not originally have. 

On the other side, in the JS core, we have custom events. These are completly different and are events that you can create for yourself. They are created in an area called the 'Event Emitter'. 

Sometimes system events are sent on as custom events, so that they appear to be the same thing. But, they are not. 

The js side is actually faking it. They are not real events. JS has no event concept or object. We create our own event library with the techinique that the node event emitter uses. 

One key to understanding the event emitter is recalling that we can use strings to dynamically access changing properties in an object:

    var obj = {
        greet: 'Hello'
    }
    // dot notation using the property name / key
    console.log(obj.greet);  // Hello

    //square bracket using the property name / key
    console.log(obj['greet']) // Hello

    // square brackets using a variable to hold the property name
    var prop = 'greet';
    console.log(obj[prop]); // Hello


You also need to recall that arrays are collections, and since functions are first class objects they too can be included in an array.

    let arr = [];

    arr.push( () => {
        console.log('1');
    });

    arr.push( () => {
        console.log('2');
    });

    arr.push( () => {
        console.log('2');
    });

    arr.forEach((fn) => {
        fn();
    })

    // 1
    // 2
    // 3

In this emiter example we will build our own simple version of the node event emitter. We will be able to see that an event has happened and then repond to it. 

Inside the ee we will have an `on` method. 'on' is a common name for an event listener because it reads nicely. EG I might say 'on a file being opened' or 'on a message being recieved'.

The on method then takes two arguments. One is for the type of event and the other is an event listener.

'Event Listener': code that responds to an event. In JS the event listener code is usually a function. When the event happens this code is invoked. You can have more than one listener for the same event. 

emitter.js:

    // could also use a class contructor to make this object
    // we want to be able to create multiple emitters.
    function Emitter() {
        this.events = {};
    }

    // add an "on" method to the protoype of all objects created from the function constructor
    Emitter.prototype.on = function (type, listener) {

        //if the type property already exists then good, otherwise make a new array.
        this.events[type] = this.events[type] || [];
        
        // now push the listener function into the array
        // we are building up an array of functions. One array for each event type.
        // onBlah: [function() {...}, function() {...}, ... }
        this.events[type].push(listener);
    }

    // now we want to say that something happened; we emit an event.
    Emitter.prototype.emit = function (type) {
        // If I have the type of event on my object, i'll loop over the 
        // associated array and execute each listener function
        if(this.events[type]) {
            this.events[type].forEach(listener => {
                listener();
            })
        }
    }

    module.exports = Emitter;



app.js:

    const Emitter = require('./emitter');

    let emtr = new Emitter();

    // Add some listeners

    // say whenever a greet happens we want to do something.
    emtr.on('greet', () => {
        console.log('Somewhere, someone said hello.');
    });

    // And this is another thing to do when a greet occurs
    emtr.on('greet', () => {
        console.log('A greeting occured!');
    });

    // simulate the event:
    console.log('Hello!');

    // let the application know that a greet event happened
    emtr.emit('greet');

You can check out the real event framework in lib/events.js. 'On' is an alias for `addEventListener`.

To re-write our example so that it uses the real event framework just change the import statement in app.js:

    const Emitter = require('events');

TO prevent magic strings we also create and require a config.js file:

config.js:

    module.exports = {
        events: {
            GREET: 'greet',
            FILESAVED: 'filesaved',
            FILEOPENED: 'fileopened'
        }
    }


Magic String: a string that has some special meaning in our code. They make it easy for a typo to cause a bug, and hard to track down that bug. It's just a string, not a variable. Relying on a string to be the basis for the logic in your code is dangerous.


So out final app.js is:

    const Emitter = require('events');
    const events = require('./config').events;

    let emtr = new Emitter();

    // Add some listeners

    // say whenever a greet happens we want to do something.
    emtr.on(events.GREET, () => {
        console.log('Somewhere, someone said hello.');
    });

    // And this is another thing to do when a greet occurs
    emtr.on(events.GREET, () => {
        console.log('A greeting occured!');
    });

    // simulate the event:
    console.log('Hello!');

    // let the application know that a greet event happened
    emtr.emit(events.GREET);


Aside on Object.create()

Object.create takes an object. It then makes a new object and set the argument as the new object's prototype.

    const Person = {
        firstName = "",
        lastName = "",
        greet = function() {
            console.log('Hello ' + firstName + ' ' + lastName);
        }
    }

    // john has Person as a prototype but it creates it's own name variables
    // so that the JS engine is not looking for them up the chain
    let john = Object.create(Person);
    john.firstName = "John";
    john.lastName = "Doe";

    // jane share the same prototype as john
    let jane = Object.create(Person);
    jane.firstName = "Jane";
    jane.lastName = "Doe";


Inheriting from the Event Emitter

We know about several inheritance approaches (ie ways to set up the prototype chain). Function constructors, extending a class and Object.create().

Within util.js (the utilities module in the node library), there is a method called inherits. This allows us to set up an entire node library object, like the event emitter module, as the prototype of one of our own contructors. That is, a node module becomes the prototype of a constructor. The first argument is your custom constructor and the second is the module that will become the prototype.


Under the hood it looks a little like 

    myConstructor.prototype = Object.create(superConstructor.prototype);


app.js:

    var EventEmitter = require('events');
    var util = require('util');

    function Greetr() {
        this.greeting = 'Hello World!';
    }

    // The prototype for objects created from EventEmitter will also be the prototype of the prototypes
    // created for all objects created from Greetr
    util.inherits(Greetr, EventEmitter);

    // You can still as your own methods and properties
    Greetr.prototype.greet = function(data) {
        console.log(this.greeting + ': ' + data);

        // And you can access EE methods too (which are on 'this' - 
        // the created object- don't call EventEmitter.emit because
        // EventEmitter is a constructor and not your object)
        // Here we use an optional argument in emit. Emit automatically
        // passes extra arguments into the event listeners
        this.emit('greet', data);  // normally don't use a magic string
    }

    const greeter1 = new Greetr();

    greeter1.on('greet', function(data) {
        console.log('Someone greeted: ' + data);
    })

    greeter1.greet('Tony');

Here is the same code converted to es6 (the call to super is discussed further below).

greetr.js:

    'use strict';

    var EventEmitter = require('events');

    // use `extends` instead of util.inherits(Greetr, EventEmitter);
    module.exports = class Greetr extends EventEmitter {
        constructor() {
            super(); // get specific object properties
            this.greeting = 'Hello World!';
        }

        greet(data) {
            console.log(this.greeting + ': ' + data);
            this.emit('greet', data);  // normally don't use a magic string
        }

    }

app.js

    var Greetr = require('./greetr');

    const greeter1 = new Greetr();

    greeter1.on('greet', function (data) {
        console.log('Someone greeted: ' + data);
    })

    greeter1.greet('Tony');



This is a key concept in node. Many objects in node can do many things plus listen for and emit events.

Here is another example that waits for async random number requests to be generated.

numEvents.js:

    // extends the events lib to handle random number events
    const util = require('util');
    const EventEmitter = require('events');
    const eventConfig = require('./config2').events;

    function NumEventer() {
        // add in a call to the super constructor if you also need some 
        // properties generated only in that constructor
        // EventEmitter.call(this);
    }

    const numRequest = function() {
        setTimeout(() => {
            const randomNum = Math.random();
            this.emit(eventConfig.NUM_REC, randomNum);
        }, 2000);
    }

    util.inherits(NumEventer, EventEmitter);

    NumEventer.prototype.on(eventConfig.NUM_REC, function(numData) {
        console.log(`Your percentage is: ${(numData*100).toPrecision(2)} %`);
    });

    NumEventer.prototype.request = function() {
        numRequest.call(this);
        console.log('Sent a request for random number');
    };

    module.exports = NumEventer;

app.js

    const NumEventer = require('./numEvents');
    const numEventer = new NumEventer();

    // request a random number from the "net"
    numEventer.request();


Here is another example of utils along with complete inheritance. This a common pattern in nodejs

    const util = require('util');

    function Person() {
        this.firstName = "John";
        this.lastName = "Doe";
    }

    Person.prototype.greet = function() {
        console.log(`Hello ${this.firstName} ${this.lastName}`);
    }

    function PoliceOfficer() {
        // Because this is a function constructor an empty object is assigned to 'this'
        Person.call(this); // attach properties to `this` from the super constructor
        this.badgeNumer = 1234;
    }

    // make the prototype of the prototype for police objects the same prototype
    // that Person objects have.
    util.inherits(PoliceOfficer, Person);

    const officer = new PoliceOfficer();

    // name properties inherited
    officer.greet(); // Hello John Doe

    // greet is on the proto of the proto!
    officer.__proto__.__proto__.greet(); // Hello undefined undefined



Aysnchronous Code, Streams and Buffers
-------------------------------------------

Aysnchronous: more than one process is running simultaneously. JS and v8 are synchronous (one process or one line of code running at a time). Node does things asynchronously. When we think about synchronisity we really have to think about what a js engine is sitting inside of. It is a browser or is it node?

First recall what a callback is: a function passed to another function, which we asume will be invoked at some point. The function 'calls back' invoking the function you gave it when it is done with its work.

We now look at the c++ side of events in node with libuv, with the event loop and non-blocking code. We have already seem custom events with the event emitter. This was just a trick. Real system events are handled in the c++ core by a library called libuv. This manages events from the operating system. It is closer to the machine. 

libuv sends requests to the operating system. Examples of these include opening files, database access, and requesting data from the internet. Responses back from the OS are placed in a queue of events within libuv. libuv constantly checks to see if something is in the queue. Sometimes, between checks, multiple events may have been added to the queue. When libuv does see that something is complete it processes it by running the associated callback back in v8. This is code that is meant to run when the libuv event loop completes (request --> event to queue --> event processed). If you dig into the c code within linuv's core code you actually see a while loop.

V8 is synchronous, so any work related to the callback is again placed in v8's own queue of work. However, this entire process is aysnchronous. libuv is running it's own event loop while v8 is carrying on with whatever work it has to do. So this why node is often referred to as 'non-blocking'; node can get on with other work while waiting for some operation to complete. All your javascript code keeps working, and you app does not freeze up, while you wait for a request. 


Streams and Buffers

A buffer is a temporary holding place for data while that data is being moved from one place to another. It is intentionally limited in size so that the temporary data is not stored for long. Usually this data is handled by an object called a 'stream'.

A stream is a sequence of data moving over time. Within it are pieces of data that are eventually combined into a whole. 



Buffers

Traditionally JS was not very good at encoding. That is, dealing with pure binary data. Such work was more related to server side jobs. So node expands JS and allows us to deal with binary data.

Binary data: data is stored in binary (aka base 2). Each 1 (high V) or 0 (low V) is called a 'bit' or 'binary digit'.

Character set: a representation of characters as numbers. Each character gets a number. Unicode and ASCII are character sets.

Character encoding: how characters are stored in binary. The numbers (or 'code points') are converted and stored in binary numbers.

UTF-8 is a popular character set because it encodes into 8 bit numbers, which allows for many characters.

The node API details a built in buffer object. It mas many methods for dealing with binary data. You can choose a number of encoding syles.

Ultimately a node buffer object holds raw binary data, but it allows us to change what you see with encoding. 

    // buffer is so fundamental that a 'require' is not necessary
    var buffer = new Buffer('Hello', 'utf-8'); // encode a buffer object with the given string with utf-8

    console.log(buffer);   // <Buffer 48 65 6c 6c 6f>  - more readable hex representation of the binary data
    console.log(buffer.toString());  // Hello is produced with utf8.
    console.log(buffer.toJSON());  // { type: 'Buffer', data: [ 72, 101, 108, 108, 111 ] } // unicode base 10 representation
    console.log(buffer[1]); // 101 - the unicode decimal number for e. 

    // overwrite the buffer, which was initialised with only five characters
    buffer.write('wo');
    console.log(buffer.toString()); // wollo

Often you won't directly deal with the buffer. Mostly a buffer object is coming back from some other utility or feature in nodejs. 


Typed arrays;

This is a feature embedded into the v8 engine. It is not actually nodejs. Typed arrays allow the use of array methods with binary data in the buffer. If you change the array you are changing the buffer. If you read from the array you are actually reading from the buffer. 

These typed arrays have various structures (or types!).

    // storing raw binary data: 64 bits
    var buffer = new ArrayBuffer(8); // give the size in bytes, not bits

    // here we choose the int32 type
    var view = new Int32Array(buffer);

    view[0] = 5;
    view[1] = 15;
    console.log(view);  // Int32Array [ 5, 15 ]

    // our 8 byte buffer does not enough capacity for another number
    view[2] = 30;
    console.log(view);  // Int32Array [ 5, 15 ] - no change



Aside on callbacks and databases:

You might be collecting information from a database, but, depending on the situation, you might also like to do different things with that data. One approach is to pass in callbacks that take in the same response data but deal with it in different ways.

    function greet(callback) {
        console.log('hello');
        var data = {
            name: 'John Doe'
        }
        callback(data);
    }

    function logIt(data) {
        console.log('The callback was invoked');
        console.log(data);
    }

    function greetName(data) {
        console.log('A different callback was invoked');
        console.log(data.name);
    }

    greet(logIt);

    greet(greetName);

    // hello
    // The callback was invoked
    // { name: 'John Doe' }
    // hello
    // A different callback was invoked
    // John Doe


Async code example with files and the fs module (file system):

readFileSync reads a text file and returns a string. Internally it loads the contents of a file into a buffer. You can specify the encoding in which you would like to get your string back too. 

Recall that __dirname is a argument made for the IIFE that wraps a module.  It gives the path to the directory in which we are running code.

greet.txt:

    Hello world!

app.js:

    const fs = require('fs');
    const greet = fs.readFileSync(__dirname + '/greet.txt', 'utf-8'); // utf-8 encoding specified
    console.log(greet);  // Hello world!


If the encoding option is specified then this function returns a string. Otherwise it returns a buffer.

Notice 'Sync' in the method name. This tells that all code waits until the file has been read to the buffer and then returned back as a string. In most cases you do not want your code to be synchronous. For example you might have many users and you do not want them to be blocked. The file might be vary large and so require a large buffer or a long stream, which takes time. fs.readFile is async and requires a callback. As in the normal event-loop case, the callback is run once libuv has recieved a response from the OS. 

note: 'error first callbacks' are popular in node. The first argument will be null if there is no error. 

    const greet2 = fs.readFile(__dirname + '/greet.txt', function(error, data) {
        console.log('data inside readfile callback: ', data);
    });

    console.log(greet2);

    // undefined - the 2nd log ran before the data was ready
    // data inside readfile callback:  <Buffer 48 65 6c 6c 6f 20 77 6f 72 6c 64 21>  // binary buffer data

So we saw the binary data in the buffer (logged as hex to make it more readable), but we can see this as a string by using toString(). And while utf-8 encoding is the default we'll specify it again:

    const greet2 = fs.readFile(__dirname + '/greet.txt', 'utf-8', function(error, data) {
    console.log('data inside readfile callback: ', data.toString());
    });

    console.log(greet2);

    // undefined
    // data inside readfile callback:  Hello world!

Allways use async when you can because it is a better experience for your users. 



Streams

[An object for managing data as it passes through a buffer object.]

The buffer lives in a place called the heap. This is some memory allocated to the program. If you had many users all requesting the same file at the same time then you may have problems.

Chunk: a piece of data being sent through a stream. Usully limited to the buffer size.

If you dig into the stream.js library file you can see that a stream object is an event emitter (it uses util.inherits to extend EE). When certain events have occured code is then run. 

The shared stream object at the base of these types is an 'abstract' or base class. Abstract classes are those you never work with directly but instead always only inherit from into a new custom object. This means that we are provided with base ideas about what a stream should be but we have to implement how exacty how we deal with the information we expect to recieve. In saying this however, nodejs provides several implementations of stream objects; readable (only read the data and you can't send data back), writable (send data but you can't read data from the stream), duplex (read and write data), transform (change the data as it moves through the stream), and passthrough. 

So in terms of a prototype chain we might have something like 

    EventEmitter.prototype --> Stream.prototype --> Stream.Readable.prototype --> CustomStream

When we talk about readable and writable we have to remember we are talking about these concepts from node's perspective. There may be another entity doing it's own thing at the other end of a data stream (as opposed to a stream object), such as a webserver or a browser. For example a node server will perceive that requests from a browser are readable but return data is writable.

Since streams are abstract we need a concrete implementation as an example. One is the fs module. fs.readStream. This is a specialised type of read only stream object which follows the above prototype diagrammme.

So in this example there is a text file, greet.txt, which has 10's of kBytes of data. Once files get larger they get bigger than the buffer you'll only get pieces, or 'chunks' of the text file at any one time. This means that same 'data' event will be fired for each chunk, so that each associated event listener will be invoked multiple times. The default buffer size is 64 kBytes.

    const fs = require('fs');

    var readable = fs.createReadStream(
        __dirname + '/greet.txt',
        { 
            encoding: 'utf8',          // will mean that a console.log is a string and not hex
            highWaterMark: (32*1024) // override the default with 32 kBytes.
        }
    );

    // check out the docs for the correct event type; 'data'
    // here 'chunk' is the buffer.
    readable.on('data', function(chunk) {
    
        // If no encoding is specified this is a hex representaiton of the binary.
        // Each hex number is then a byte summary so that the length is the byte count.
        // console.log(chunk);

        console.log(chunk.length); // will be less that of the buffer's size in bytes (32 kBytes here)
    });

    // 32768
    // 9084


Now let's write the data to another file.

    const fs = require('fs');

    var readable = fs.createReadStream(
        __dirname + '/greet.txt'
    );

    // create a writable stream object
    var writable = fs.createWriteStream(__dirname + '/greetCopy.txt');

    readable.on('data', function(chunk) {
        writable.write(chunk);
    });

[now we have seen several ways of reading data from a file; readFileSync, readFile, and createReadStream]



Pipes

Reading and then writing, as in the previous example, is such a common pattern that there is actually a common term for it; 'pipes'.

Pipes: A pipe connects two streams by writing one stream with what is being read from another stream. In node you pipe from a readable stream to a writabe stream.

We can attach several pipes together and send data to multiple writable streams. 

Readable objects have a pipe method attached: it looks like function pipe (destination, pipeOptions) {...}, where destonation is a writable stream. Under the hood, it's just a more robust version of what we did above. This function also returns the destination writable stream. This helps us to write our code in a cleaner way.

    const fs = require('fs');

    var readable = fs.createReadStream(
        __dirname + '/greet.txt'
    );

    // create a writable stream object
    var writable = fs.createWriteStream(__dirname + '/greetCopy.txt');

    readable.pipe(writable);  // [event listener created for us]

Now, if the streams are duplex (readable and writable), then we can use the return value to work with multiple streams. 

Here we will use zlib, which is part of the node core. This allows us to implement a gzip file (a common file compression algorithm). We will copy the contents as before but also create a compressed verion. Here we use zlib.createGzip(); to create a duplex stream. Since that stream is thus also writable, the pipe method will return a stream of compressed data, which can then be writen to file.  This is called chaining.

Chaining: a method returns an object so that we can keep calling methods. If the parent object is returned then it's called cascading.

    const fs = require('fs');
    const zlib = require('zlib');

    var readable = fs.createReadStream(
        __dirname + '/greet.txt'
    );

    var writable = fs.createWriteStream(__dirname + '/greetCopy.txt');

    // a stream of compressed data to a file
    var compressed = fs.createWriteStream(__dirname + '/greetCopy.txt.gz');

    // a transformative stream; creates compressed data
    var gzip = zlib.createGzip(); 

    // Cannot chain from 'writable' because it's not readable
    readable.pipe(writable);

    // three streams chanied together here: 'readable', 'gzip', 'compressed'.
    readable.pipe(gzip).pipe(compressed);


Node's memory and speed is optimised for streams, so we should always be thinking about when we can use streams. Always consider streams and async methods over sync methods.









HTTP and Web server
-------------------------------------------


TCP/IP - how information is actually transferred

Protocol: an agreed set of communication rules. 

IP: Internet Protocol. Each computer on the internet is assigned a unique IP address, which is a sequence of numbers. So this protocol is the one in which we agree on how we identify computers. 

A socket is the line in which information flows betweent two connected computers. On the internet we open and close sockets all the time. A 'web socket' is kept open all the time.

TCP is the protocol for how information is sent (as distinct from how it is structured). TCP tells us to split up information into pieces and send each piece one at a time through the socket. Each individual piece is called a packet.

TCP is similar to a stream, and this is the way node treats internet data. 

Information is structured within it's own protocol -  a set of rules for how it should be structured. This might be http, ftp, or smtp, etc (the p stands for protocol). See more below.

Port: an OS assigns a unique number called a port to a program. When a computer then receives a packet it can be sent to the correct program. When a program is set up on the operating system to receive packets from a particular port, it is said that the program is listening to that port. For example, a server might be running both node and a separate email server. Assigning these various ports is called mapping. 

The port is specified as part of the IP address. Here the port, 443, is separated from the main IP address with a colon: 78.132.160.4:443. When you put the full address altogether it's called a socket address. Normally we just use a domain name that maps to a socket address. 

HTTP: Hypertext transfer protocol a set of rules for the format of data on the internet. It is actually use not just for hypertext (HTML) but for other information types as well, like javascript files and images. 

Headers: name:value pairs separated by a colon. 

request:

    CONNECT www.google.com:443 HTTP/1.1
    Host: www.google.com
    Connection: keep-alive

response:

    HTTP/1.1 200 OK
    Content-Length: 44
    Content-Type: text/HTML

    <html><head>...</html></head>

The first line in the response is the 'status' or 'status line'. Then come two headers. After a space is the 'body'.

Here the content type is a MIME type. 'Multipurpose Internet Mail Extensions' is a standard for specifying the type of data being sent. It was originally created for sending attachments via email. Other MIME types include application.json and image/jpeg. The browser will know how to treat the object based on it's MIME type. 

So how do we build http requests and responses? We have to parse them, which means to break them up into pieces and then use those pieces. This then, comes down to formatting. There is a program inside nodeJS that does this: http_parser. It breaks up HTTP messages and breaks them up ito separate pieces of data, like status codes, so that we can see them and make decisions with them. 

It's actually a standalone module made in C code but it is wrapped up in node and given extra features. See http.js, which is the actual module that we will import and create servers with. 


Building a Web server

Here we import the http module and use it's createServer method. That method takes a callback that is actually used as an event listener for a particular event that the server will emit. If you look inside the relevant node sub module, _http_server.js, you can see that the event type is 'request', and the callback is passed two objects. One represents the request and the other is a stream representation. This is the stream in which the response is sent back. Usally the arguments are named 'req' and 'res'.

You'll also see the callback using writeHead(). It first specifies the response and then uses an object for the headers. Note that you need quotes for the keys because some http headers are not valid javascript variable names.

After creating a server object we then need to map a port to it so that it can listen for requests. We do this with the listen method. 

    var http = require('http');

    http.createServer(function(req, res) {

        res.writeHead(200, {
            'Content-Type': 'text/plain'  // MIME for simple plain text
        });

        res.end('Hello World\n'); // Always end text data with a new line.

    }).listen(1338, '127.0.0.1'); // use port 1337 on localhost.

(To run this use the normal <node app.js> command in the command line but then open up a browser, which are really designed to build http requests and understand responses, and use localhost:1337 as the address. )

You can go to the network tab in the dev tools and see the details of the reqest and response.  There are extra headers in both cases. Node will automatically generate some extra headers in the response, and the browser adds in quite a few to the request. 

You can also run the debugger in vs vode while also placing break points in the browser, which can be very handy. In the VS Code debugger you will see various obejts like the request and the response. Within those you can see the socket, the headers, and that we are dealign with streams and event emitters. (hit refresh on the browser since the callback only runs when a request is recieved).

If you wanted to send an html file require fs and add change the content of createServer:

   res.writeHead(
      200, 
      {
         'Content-Type': 'text/html'
      }
   );

   var html = fs.readFileSync(__dirname + '/index.html');
   res.end(html);

You'll notice that if you were to now update writeHead (eg change the MIME type back to text/plain so that you see the contents of the file) and then refresh the browser that nothing changes in the browser. This is because nothing has told node to recompile your JS code into machine code. However you can change the html file itself. That file is not compiled but sent as it is with each request.

What about text on a webpage that is dependant on a variable? Often devs will use a template (usually there is a specific template language so that the system knows how to replace placeholders with real values). So in this example we replace some text with what we call a 'Dynamic Template':

html file:

    <body>
        <h1>{message}</h1>
    </body>

in App.js:

    http.createServer(function(req, res) {

    res.writeHead(
        200, 
        {
            'Content-Type': 'text/html'
        }
    );

    // here we need a string to manipulate before we send onto the stream so we use utf8 encoding
    var html = fs.readFileSync(__dirname + '/index.html', 'utf8');
    var message = 'Hello World...';
    html = html.replace('{message}', message);
    res.end(html);

    }).listen(1337, '127.0.0.1'); // use port 1337 on localhost.

In the last couple of examples we used readFileSync. Particularly for a busy website, it would be a lot more efficient to swap that out with a stream. We can do that by creating a read strem with the html file and then directly piping the html data into the response stream already created for us createServer.

This means that our buffer will be small and our application will run faster and more performant. 

    var http = require('http');
    var fs =  require('fs');

    http.createServer(function(req, res) {

        res.writeHead(200, {
            'Content-Type': 'text/html'
        });

    fs.createReadStream(__dirname + '/index.html').pipe(res);

    }).listen(1337, '127.0.0.1');



API Endpoints and Outputting JSON

A web based API is usually available through a set of URLs which accept and send data via http and TCP/IP. The URL is called an 'endpoint' (one URL in a web API). Sometimes an endpoint will be able to do multiple things, with the choice based upon the http request headers. 

We can adjust our simple example to just display some JSON in the browser:

    var http = require('http');

    http.createServer(function(req, res) {

        res.writeHead(200, {
            'Content-Type': 'application/json'
        });
        var obj = {
        firstName: 'John',
        lastName: 'Doe'
        };

    res.end(JSON.stringify(obj)); // stringfy for browser display in this example

    }).listen(1337, '127.0.0.1');

We could also imagine that some javascript running in the browser has made a request for some JSON data and then use it. The borwser could send a request to an API end point, he object on the server could be converted to JSON (ie serialized), send to the browswer, and then deserialised.

Serialize: translating an object into a format that can be stored or transferred. JSON, CSV, XML and others are popular. 'Deserialize' is the opposite: converting the format back into an object.



Routing


Routing involves mapping http requests to content. The actual files may exist on the server or not. Routing is all about responding to the request. 

At the moment, with our JSON example above, we can use any URL path and still get the same result. For example:

    http://localhost:1337/someFile.html

will still just display the JSON string in the browser. That is, we are still getting the same data. We need to provide content based on the URL. We can access the URL with req.url.

    http.createServer(function(req, res) {

    // standard request
    if(req.url === '/') {
        fs.createReadStream(__dirname + '/index.html').pipe(res);
    }

    else if(req.url === '/api') {
        res.writeHead(200, {
            'Content-Type': 'application/json'
        });
        var obj = {
            firstName: 'John',
            lastName: 'Doe'
        };
    
        res.end(JSON.stringify(obj));
    }

    else {
        res.writeHead(404);
        res.end();
    }

    }).listen(1337, '127.0.0.1');






Node Package Manager
------------------------------------------------

NPM gives you access to the largest ecosystem of free software in history.

A package is a just a collection of code. It is managed and maintained with a package management system. 

Package management system: software that automates installing and updating packages and their dependencies. 


Semantic versioning (semver)

Semantic implies that something has meaning. The version number is based on rules so that just looking at the verison number should give us some information. 

    major.minor.patch
 eg
    1.7.2

Patch: some bugs were fixed and are backwards-compatible. Your code should still be okay after a updating this dependency.

Minor: Some new features were added. Add functionality in a backwards-compatible manner. Code should still be okay.

Major: Big changes such as incompatible API changes. Your code may break.

Software using semantic versioning MUST declare a public API.

For more see https://semver.org/ 


NPM

The npm system is installed along with nodejs. It should not be confused with the npm registry, which is the location of the packages. The registry is at npmjs.com. You can go there to check out the contents of packages. 

Anyone can contribute to the registry so that means that the quality is not always great. Check out any packages you might install. Check for open issues, number of contributors and how many recent downloads there have been.



INIT, Nodemon, package.json

To set up npm in a project:

    $ npm init 

The project name should be something unique because your project may end up in the registry.

    name: nodejs-test-app
    version: 1.0.0 // auto generated in brackets
    description: Nodejs Test App
    entry point: app.js

You will also need to specify the other fields if you plan to publish this to the registry. 

This generates a JSON file called package.json. 

Now say we want to install moment - a popular package for date and time. 

    npm install moment --save

npm will then download the moment files from the registry and make a module available to us in our app. 

The --save CLI argument saves the dependency to package.json 

This also creates a new node_modules folder (consider adding to .gitignore).

In package.json we see that moment is listed in a dependencies object:

  "dependencies": {
    "moment": "^2.23.0"
  }

The caret means that we should not update to a new major verson. Otherwise if there are later minor and patch versions then moment will be automatically updated.

If you have a tilde then it means only do patch updates.

  "dependencies": {
    "moment": "~2.23.0"
  }

There are a couple of advantages with package.JSON. One is that you can easily determine the dependencies of the app. The 2nd is that node packages need not be save to a repo or sent to other. Once package.json is in a file you need only use:

    $ npm install

Npm will the install all dependencies. 

To import a dependency as a module you use require but without the forward slash, as if it were a core module. If node cannot find it in the core module then it looks in node_modules. 

The dependencies so far those required for our app to run. Other packages are not required for the app to run but are instead used during the development process. 

    $ npm install jasmine-node --save-dev // aka -D

Now we have a new dev dependencies object in package.json

  "dependencies": {
    "moment": "^2.23.0"
  },
  "devDependencies": {
    "jasmine-node": "^1.16.2"
  }

There is one other kind of package that is again not for the app and only for developement. However we would like to use it across all of our projects. It is global (on Win it is stored deep within appData).

    $ npm install jasmine-node -g 

These packages do not go into node_modules but somewhere else on the machine that node can access. 

    npm install -g nodemon  // you can change the order of the arguments


Now that nodemon is installed globally we can use it in the command line. 

nodemon is short for 'Node Monitor'. It watches the files in your app. It watches files and waits for them to change. If a file changes nodemon closes the node application and runs it again in the CLI. eg node app.js:

    $ nodemon app.js

So now, for example, every time we change our server file we don't need to manually stop the sever and restart it.

Lastly, if you want to update your dependencies, subject to a caret or tilde, then run:

    $ npm update


Node scripts

package.json:

  "scripts": {
      "lint": "eslint src",
      "flow": "flow",
      "format": "npm run prettier -- --write",
      "prettier": "prettier \"**/*.+(js|jsx|json|yml|yaml|css|less|scss|ts|tsx|md|graphql|mdx)\"",
      "validate": "npm run lint && npm run prettier -- --list-different"
  },
  
In validate we see double ampersand, which means that the lint command is run and then followed by the prettier command. We also see that the prettier command is run with different flags. The format command passes in a write command (-- --write), which would make the prettier command look like:

  "prettier": "prettier --write \"**/*.+(js|jsx|json|yml|yaml|css|less|scss|ts|tsx|md|graphql|mdx)\"",  

Wheras the validate command passes in a different flag, (--list-different).

Also note that with some packages like flow, which type cehcks needs a pre compiler directive or 'pragma' on each file to be checked. For flow this is //flow at the top of the file.

Tip for eslint. The syntax that other packages use, like flow, may not be compatabile with eslint. Try adding in a parser:

  $ npm install --save-dev babel-eslint
  
.eslintrc: 
 
 {
    "parser": "babel-eslint",
    
 It is also possible to use special git hooks within npm scripts. Say you wanted to automaticlly run some static testing (like prettier) before committing. 
 
 $ npm install --save-dev husky
 
If you now inspect your git directory there's a hooks directory. These git hooks inlcude a pre-commit file. Now we just add a precommit script in package.json:

  "scripts": {
      "lint": "eslint src",
      "format": "npm run prettier -- --write",
      "prettier": "prettier \"**/*.+(js|jsx|json|yml|yaml|css|less|scss|ts|tsx|md|graphql|mdx)\"",
      "validate": "npm run lint && npm run prettier -- --list-different && npm run flow",
      "precommit": "lint-staged && npm run flow"
  },

This will run husky's precommit script. prettier or lint throw an error then your commit will fail (can be overwidden with --no-verify). note; we used lint-staged here so that we only lint check changed files and not the entire project. This is its setup:

  $ npm install --save-dev lint-staged
  
.lintstagedrc:
  
  {
    "linters": {
      "*.js": [                           // will match for any files the match the glob *.js
        "eslint"
      ],
      "**/*.+(js|jsx|json|yml|yaml|css|less|scss|ts|tsx|md|graphql|mdx)": [
        "prettier --write",
        "git add"                     // once prettier has run then run git add
      ]
    }
  }

 
 Without lint-staged the precommit script would have just been:
 
  "precommit": "npm run validate"
 
 


Express
--------------------------------------------------------

Express is one of the most popular packages on the npm registry. It's primary access point is createApplication(). This function is not a constructor but a regular function that returns a function. That function has various properties attached (functions are objects so can have properties too) such as response() and request(). We will have to invoke the function to get the return value.


    var express = require('express');

    // invoke to return our app function with many attached methods
    var app = express();

    // create a server on port 3000
    app.listen(3000);

Environment Variables: global variables specific to the environment (eg server) in which our code lives. We can access these variables in our code.

More realistically we want to use environment variables to set the port. For example, the port on a test machine may be different to the production machine. We can access environment variables through a global node object called process.

If we have a 'PORT' variable set on the server then we can access it with

    var port = process.env.PORT;

HTTP method: specifies the type of action the request wishes to make. They are also called verbs. Examples include GET (eg a get a webpage), POST (eg submit a form; there is some data attached.) and DELETE. They do not actually do anything on their own. They are only text in the HTTP request. It is up to the server to do what is expected. That is verbs are matched with a function. 

In express these methods are already attached to the app function, eg app.get() aligns with GET verb and a particular URL. This is a better way to handle routing, for which we previously used an if statement.

The express methods also take a callback with req and res arguments. These express versions wrap the normal response and reqest with additional features. For example, we no longer need to specify the content type. Express looks at the content you provide and supplies this automatically. 

    var express = require('express');
    var app = express();

    // use default port 3000 if no ennvironment variable has been set.
    var port = process.env.PORT || 3000;

    // send an html page
    app.get('/', function(req, res) {
        res.send('<html><head></head><body><h1>Hello World!</h1></body></html>');
    });

    // send object literal syntax automatically converted to a JSON string
    app.get('/api', function(req, res) {
        res.json({ firstName: 'John',  lastName: "Doe"});
    });

    app.listen(port);


Routes

Many routing options are available. see expressjs.com. You can actually match for patterns and not just URLS. 

For example a colon tells express that a sub pattern could be anything. 

    // req.params contains the arguments generated bymatching the request URL and the pattern.
    app.get('/person/:id', function(req, res) {
        res.send('<html><head></head><body><h1>Person: ' + 
        req.params.id + '</h1></body></html>');
    });




Static Files and Middleware

Much of the work in node happens between the request and the response. However there are many common scenarios in this area. 

Middleware: code that sits between two layers of software. In the case of Express, we are talking about code between the request and the response. Esentially get() is middleware too but Express has an easy way to plug in any middleware. 

A common situation is handling request for a file download. For example if a css file has been requested by the browser then we would hope that such static files are automatically dealt with. (Static files that are not processed or generated by code, eg HTML, CSS and image files).

Static files are often place in a top level directory called 'public'. Here we have style.css in that folder:

    // any URL that begins with '/assets' will be handled here.
    app.use('/assets', express.static(__dirname + '/public'));

    // The css link specifies 'assets' so a request for such will be sent by the browser
    app.get('/', function(req, res) {
        res.send('<html><head><link href="assets/style.css" type="text/css" rel="stylesheet"/></head><body><h1>Hello World!</h1></body></html>');
    });

We can make our own middleware. Generally app.usesimply takes a url and a function. That function then handles the request and repsonse and a 'next' parameter.

    app.use('/', function(req, res, next) {
        console.log('Request URL: ' + req.url);
        //invoke this callback. It just means run the next middleware after completion
        next();
    });

There are a large number of 3rd party middleware packages designed for express. You can see the list at https://expressjs.com/en/resources/middleware.html . For example you can see cookie-parser. Browser have cookies attached that relate to certain sites. If an http request is made to a site, then any associated cookies are sent with the request. This middleware will parse those cookies.  

Another popular middleware package is passport, which checks that people are properly logged in. 



Templates and Template Engines - Dynamically Generating HTML 

Express has a number of available template engines. A template engine enables you to use static template files in your application. At runtime, the template engine replaces variables in a template file with actual values, and transforms the template into an HTML file sent to the client. This approach makes it easier to design an HTML page.

See https://expressjs.com/en/guide/using-template-engines.html 

A template engine essentialy takes text and produces an html file. Express is 'unopionated', which means that it does not promote one plugin over another. One simpler engine is EJS (ejs.co). It takes delimiters;  <% and %>, and an out which is an denoted by an equals sign. Anything between the <% %> is js code. Here user.name is outputted as a string within h2 tags:

    <% if (user) { %>
    <h2><%= user.name %></h2>
    <% } %>

To use it:

    $ npm install ejs --save

And introduce it as middleware in app.js:

    app.set('view engine', 'ejs');

By default ejs looks for static files in a top level directory called views. This folder is generally for your user interface, or your 'view'. So here, inside views, we have index.ejs:

    <html>
        <head>
            <link rel="stylesheet" href="assets/style.css" text="text/css">
        </head>
        <body>
            Hello World from index.ejs
        </body>
    </html>

In app.js we now update our get method to use a render method. render looks to our set method (above) and checks which view engine has been set. It then takes the string argument we have suppled render (here it's index) and attaches the file extension argument we supplied in set() - 'ejs'. This is good because if we ever change view engines then we only need to change the extension in one place.

    app.get('/', function(req, res, next) {
        res.render('index');
        next();
    });

Now we can have a file and use it as a template. We can pass in arguments for the template via an object as the 2nd argument of the render method.

person.ejs (note the extra leading '/' in href. This is an easy to miss bug - while working in the views folder our path gets messed up. We have to go back to the project root)

    <html>
        <head>
            <link rel="stylesheet" href="/assets/style.css" text="text/css">
        </head>
        <body>
            <h1>Hello from ejs</h1>
            <h1>Person <%= ID %></h1>
        </body>
    </html>


app.js:

    app.get('/person/:id', function(req, res, next) {
        res.render('person', { ID: req.params.id });
        next();
    });




Querystring and Post Parameters

Query strings appear in the URL. EG:

    www.randomPage.com/$id=4&page=3

When the browser builds a GET request it places the query string within the header of the request, just after the 'GET'. 

    GET/$id=4&page=3 HTTP/1.1

You would normally then need to parse the reqest into a format we can use within our code. 

With a post, it moves into the body of the http request. You will first see a long winded content-type of application/x-www-form-urlencoded. In the body:

    username=Tony&password=pwd

With JSON data then the body is a JSON object in the body. 

    {
        "username":"Tony",
        "password":"pwd"
    }

In the GET case Express does have the querystring sitting on the request object.

    req.query.queryKey

For example we can extend the previous example with person.ejs.

Add an additional h2 in person.ejs:

    <h2>QueryString: <%= Qstr %></h2>

Also add an extra argument in the get handler app.js using req.query:

    app.get('/person/:id', function(req, res, next) {
        res.render('person', { ID: req.params.id, Qstr: req.query.qstr});
        next();
    });

Now just type in a URL with a query string:

    http://localhost:3000/person/Bob/?qstr=123

And this then displays an h2 with `QueryString: 123`


However, for the POST and JSON cases we will need to parse the body of the request. The middleware module is called body-parser. 

    npm install body-parser --save

app.js:

    var bodyParser = require('body-parser');

index.ejs:
    <html>

    <head>
        <link rel="stylesheet" href="assets/style.css" text="text/css">
        <script src="https://code.jquery.com/jquery-3.3.1.js" integrity="sha256-2Kok7MbOyxpgUVvAk/HJ2jigOSYS2auK4Pfzbm7uH60="
            crossorigin="anonymous"></script>
    </head>

    <body>
        <form action="/Person" method="POST">
            First Name: <input type="text" id="firstname" name="firstname" />
            Last Name: <input type="text" id="lastname" name="lastname" />
            <input type="submit" value="submit" />
        </form>
        <script>
            $.ajax({
                type:"POST",
                url: "/personjson",
                data: JSON.stringify({firstname: 'Jane', lastname: "Doe"}),
                dataType: 'json',
                contentType: 'application/json'
            })
        </script>
    </body>

    </html>

app.js:

    // create application/x-www-form-urlencoded parser
    var urlencodedParser = bodyParser.urlencoded({ extended: false });

    // create application/json parser function to be middleware
    var jsonParser = bodyParser.json();

    //use the body-parser as a first callback (we can have multiple callbacks)
    app.post('/person', urlencodedParser, function(req, res, next) {
        res.send('Thank You!');

        // urlencodedParser will add a body property to req
        console.log(JSON.stringify(req.body), req.body.firstname, req.body.lastname);
        // {"firstname":"Bob","lastname":"Jones"} Bob Jones
    });

    //use the a middlware callback for parsing JSON
    app.post('/personjson', jsonParser, function(req, res, next) {
        res.send('Thank you for the JSON data');

        // jsonParser will add a body property to req
        console.log(JSON.stringify(req.body), req.body.firstname, req.body.lastname);
        // {"firstname":"Jane","lastname":"Doe"} Jane Doe
    });



Restful APIs and JSON made Easy with Express

REST is an architectural style for building APIs. It stands for 'Representational State Transfer'. We decide that HTTP verbs and URLs mean something. We want to standardises how we treat verious verbs on the server so that what happens matches our expectations. 

A simple example of a restful API looks like:

    app.get('/api/person/:id', function(req, res) {
        // get some data from the database
        res.json({ firstName: 'John',  lastName: "Doe"});
    });

    app.post('/api/person', jsonParser, function(req, res, next) {
        // save to the database
    });

    app.delete('/api/person/:id', function(req, res) {
        // delete a person from the database
    });

With express we can see the URL and the verb together so that we can better see what is going on. A good restful API should use the http verbs and respond as expected. 



Structuring an App in Express

In real world applications one app.js file is not enough. We use a module approach, and because Express is 'unopinionated' there are many available options. One possiblity is express-generator:

    https://expressjs.com/en/starter/generator.html

    $ npm install express-generator -g

After installing navigate to a new project folder and run:

    $ express myAppName

A standard set of folders such as routes and views folders are created. 

Note that some middleware is used for routes and users. These are two local modules.

app.js

    var indexRouter = require('./routes/index');
    var usersRouter = require('./routes/users');

If you look inside, say the users module, you see express.Router();

This is router middleware. Instead of having get and post methods on the app, as we did above, we put them on the router.

users.js:

    var express = require('express');
    var router = express.Router();

    /* GET users listing. */
    router.get('/', function(req, res, next) {
    res.send('respond with a resource');
    });

    module.exports = router;

This is used inside app.js to break p and organise the number of routes:

    app.use('/', indexRouter);
    app.use('/users', usersRouter);


Another way to break up your project is to just make a top level directory called 'controllers'. Within that you might have files like apiController.js and htmlController.js.
We then just pass the app object around ad add features to it.

apiController.js:

    module.exports = function(app) {
        
        app.get('/api/person/:id', function(req, res) {
            // get some data from the database
            res.json({ firstName: 'John',  lastName: "Doe"});
        });
        
        app.post('/api/person', jsonParser, function(req, res, next) {
            // save to the database
        });
        
        app.delete('/api/person/:id', function(req, res) {
            // delete a person from the database
        });
    }


app.js:

    var apiController = require('./controllers/apiController');
    apiController(app);





Javacript, JSON and Databases
--------------------------------------------------

Relational Databases and SQL

Relational databases are normally thought of as tables, with fields as the column titles and rows of data. However in properly formatted relational databases we try not have all data in one table. For example instead of a table of contacts we might have table of people and then a table of addresses. Successive layers are usually linked by an ID. 

People table fields: ID, FirstName, lastName
PersonAddresses table fields: PersonID, AddressID
Addresses Table: ID, AddressID

This would be 'well normalised data', which means data that does not repeat itself unneccesarily.

Asking questions about our data is often done with SQL. It can combine tables and get the information we want. 

In Javascript we can think of a table as an array or objects. We need something that can connect to a database and convert data into a javascript object. 



Node and mysql

One several possible sql packages.

    $ npm install mysql --save

Check the documentation but basically you create a connection object to the database and then send it string requests. A javascript array of js objects is returned.



NoSQL and Documents

NoSQL is a variety of technologies that are alternatives to tables and sql. One of those is the 'document' database. MongoDB is one of those. Table is not stored in tables. The database just stores the js array of objects. Keys are repeated and use more space, but harddrive space is cheaper now. This gives us much more flexibility, such as changing our structure on the fly. 



MongoDB abd Mongoose

You can go to mlab.com and set up a free cloud based database. This will give a URL that you can plug into mongoose. Then set up a 'schema', which describes your objects or the document. 




The MEAN Stack - MongoDB, Express, AngularJS and Nodejs
-----------------------------------------------------

Stack: the combination of all technologies used to build a piece of software.

A full stack developer can work with all of the stack (client side software like Angular/React, server side like node, database like MongoDB and maybe even design).




Browsers

Like node, browsers are also written in C++ and they sometimes create features they go beyond the JS specification. But how does a browser break down data into a webpage? That is to do with the DOM.

DOM: the Document Object Model. The structure that brwosers use to store and manage web pages. Browsers five the javascript engine the ability to manipulate the DOM.

The browser is a program sitting on the client's computer. As a result of a http request is recieves some html. The html response body contains the html. The browser takes that html and converts it into a hierachy of objects, a tree, where nodes represent html elements. HTML is structured in a tree like pattern so it translates very well to the DOM. 

The browser then renders the webpage using the DOM, and not the html. 

Also embedded in the browser is the javascript engine. When the engine runs code that alter the DOM (eg event handlers adn script tags), then the web page is redrawn. 

Various browsers can require slightly different code. You can save a lot of time by using someone else's code to handle those variances and also add additional functionality. Here we use AngularJS for that purpose. Angular works on the client side in the browser while node works in the server. 

Since Angluar will will used on the client side one way to speed up your node server to get the google CDN url for Angular and place it in a script tag. The client then will load the JS file for Angular from Google rather than your server.

One of the biggest challanges to full stack development is knowing which part of your code is on the server and which parts run in the client. Understanding the interaction between the two is also important.



Testing your API

Try www.getpostman.com













[background JS refresher:

Function Constructors

A function constructor is a normal function that is used to create objects.  If you use the new keyword the 'this' variable points to a new empty object, and that object is returned from the function automatically. 

Because 'this' has been automaticaly created then we can attach things to it inside the function before it is returned:

    function Person (firstName, lastName) {
        this.lastName = lastName;
        this.firstName = firstName;
    }

    var john = new Person('John', 'Doe');

It's also possible to add to this object's prototype (see 'prototype chain' - in js inheritance is from other objects, not a class):

    Person.prototype.greet = function() {
        // use 'this' so that we refer to the object and not some variable (so don't use an arrow function here)
        console.log('hello ' + this.firstName + ' ' + this.lastName);
    }

The naming here is a little confusing. This prototype is the prototype of any object created from the Person function. It is not the prototype of the constructor function. So all objects created from this function constructor can have access to this greet method.

The js engine will now search down the prototype chain for this method:

    john.greet();


If you ever want to see the prototype for an object use __proto__ :

    // not recommended for produciton code
    console.log(john.__proto__);  // { greet: [function] } 

The es6 version of a function constructor is syntactic sugar. Classes in JS are not the same as they are in other languages.
    
    'use strict';

    class Person {
        
        // on each object
        constructor (firstName, lastName) {
            this.lastName = lastName;
            this.firstName = firstName;
        }

        // on the prototype that each object shares
        greet() {
            console.log('hello ' + this.firstName + ' ' + this.lastName);
        }
    }

    // object created the same way
    var john = new Person('John', 'Doe');

]
